{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM - Accumulated Sales of Category 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "#from sasaki_features import add_feature_position_month\n",
    "sys.path.append(\"../dora/models\")\n",
    "from utils import read_data, process_time, merge_data, dataset_builder, cumulative_sale_by_category\n",
    "\n",
    "NUMBER_OF_LAGS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sasaki_features import add_feature_position_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_score(prediction, target, simulatedPrice):\n",
    "    prediction = prediction.astype(int)\n",
    "\n",
    "    return np.sum((prediction - np.maximum(prediction - target, 0) * 1.6)  * simulatedPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval(prediction, dtrain):\n",
    "    \n",
    "    prediction = prediction.astype(int)\n",
    "    target = dtrain.get_label()\n",
    "\n",
    "    simulatedPrice = dtrain.get_weight()\n",
    "    \n",
    "    return 'feval', np.sum((prediction - np.maximum(prediction - target, 0) * 1.6)  * simulatedPrice), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(predt, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    sp = dtrain.get_weight()\n",
    "    return -2 * (predt - np.maximum(predt - y, 0) * 1.6) * (1 - (predt > y) * 1.6) * sp\n",
    "\n",
    "def hessian(predt, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    sp = dtrain.get_weight() \n",
    "    return -2 * ((1 - (predt > y) * 1.6) ** 2) * sp\n",
    "\n",
    "def objective(predt, dtrain):\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our dataset\n",
    "This notebook makes this step cleaner than the previous versions. So It'll be tidier and shorter than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itemID', 'salesPrice', 'group_backwards', 'days', 'time', 'order', 'transactID', 'days_backwards'}\n"
     ]
    }
   ],
   "source": [
    "infos, items, orders = read_data(\"../main/datasets/\")\n",
    "process_time(orders)\n",
    "\n",
    "orders_columns = set(orders.columns)\n",
    "print(orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_builder(orders, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orderSum', 'customerRating', 'recommendedRetailPrice', 'category3', 'manufacturer', 'category2', 'brand', 'category1'}\n"
     ]
    }
   ],
   "source": [
    "orders2_columns = set(df.columns)\n",
    "print(orders2_columns - orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function without information from futures\n",
    "def apply_to_serie(data, function,extraParans={}):\n",
    "    \n",
    "    new_data = pd.DataFrame()\n",
    " \n",
    "    for time in data['group_backwards'].unique():\n",
    "        new_rows = function(data,time,**extraParans)\n",
    "        new_data = pd.concat([new_data, new_rows])\n",
    "        \n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_freq(data,time):\n",
    "    \n",
    "    #a linha de baixo utiliza o orders sem row com orderm 0\n",
    "    orders_aux = orders.query(f\"group_backwards >  {time}\")#remember, its backwards\n",
    "    \n",
    "    nDays = orders_aux['days'].nunique()\n",
    "    #nWeek = orders_aux['week_backwards'].nunique()\n",
    "    nGroup = orders_aux['group_backwards'].nunique()\n",
    "\n",
    "    newInfo = items[['itemID']].copy()\n",
    "    newInfo[\"group_backwards\"] = time\n",
    "    \n",
    "    #how many days in average the item is sold in day/week/pair\n",
    "    newInfo['freq_day'] = orders_aux.groupby('itemID', as_index=False)['days'].nunique()/nDays\n",
    "    #newInfo['freq_week'] = orders_aux.groupby('itemID', as_index=False)['week_backwards'].nunique()/nWeek\n",
    "    newInfo['freq_group'] = orders_aux.groupby('itemID', as_index=False)['group_backwards'].nunique()/nGroup\n",
    "    \n",
    "    current_time = data.query(f\"group_backwards == {time}\")\n",
    "    return pd.merge(current_time,newInfo, on=['itemID','group_backwards'], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "df2 = apply_to_serie(df,  add_feature_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_min_max(data,time):\n",
    "    orders_aux = data.query(f'group_backwards > {time}')\n",
    "\n",
    "    newInfo = items[['itemID']].copy()\n",
    "    newInfo['group_backwards'] = time\n",
    "    \n",
    "    #minimun and maximum sales in a pair\n",
    "    #too much zeros, trying for last 4 pairs\n",
    "    newInfo['min_sale'] = orders_aux.groupby(['itemID'])['orderSum'].min()\n",
    "    newInfo['max_sale'] = orders_aux.groupby(['itemID'])['orderSum'].max()\n",
    "\n",
    "\n",
    "    #minimun and maximum sales in a group recent\n",
    "    order_recent = data.query(f'group_backwards > {time} & group_backwards < {time+4}')\n",
    "    newInfo['min_sale_rec'] = order_recent.groupby(['itemID'])['orderSum'].min()\n",
    "    newInfo['max_sale_rec'] = order_recent.groupby(['itemID'])['orderSum'].max()\n",
    "\n",
    "    current_time = data.query(f\"group_backwards == {time}\")\n",
    "    return pd.merge(current_time,newInfo, on=['itemID','group_backwards'], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "df2 = apply_to_serie(df2,  add_feature_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = add_feature_position_month(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage_accum_cat_3 feature...\n",
    "df3 = cumulative_sale_by_category(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'percentage_accum_cat_3', 'max_sale', 'posM_f_group', 'min_sale_rec', 'posM_l_group', 'freq_day', 'max_sale_rec', 'min_sale', 'posM_m_group', 'freq_group'}\n"
     ]
    }
   ],
   "source": [
    "orders3_columns = set(df3.columns)\n",
    "print(orders3_columns - orders2_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell lags and diffs our feature 'orderSum'\n",
    "shifting = df3.copy()\n",
    "\n",
    "for i in range(1, NUMBER_OF_LAGS + 1):\n",
    "    # Carrying the data of weeks t-1\n",
    "    shifting[f'orderSum_{i}'] = shifting.groupby('itemID')['orderSum'].shift(i)\n",
    "    shifting[f'percentage_accum_cat_3_{i}'] = shifting.groupby('itemID')['percentage_accum_cat_3'].shift(i)\n",
    "\n",
    "    \n",
    "    # Getting the difference of the orders and promotions between weeks t-1 and t-2...\n",
    "    shifting[f'orderSum_diff_{i}'] = shifting.groupby('itemID')[f'orderSum_{i}'].diff()\n",
    "    shifting[f'percentage_accum_cat_3_{i}'] = shifting.groupby('itemID')[f'percentage_accum_cat_3_{i}'].diff()\n",
    "    \n",
    "# LGBM Says on docs that it automatically handles zero values as NaN\n",
    "shifting.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding first apperance\n",
    "not_zero_order = shifting.query('orderSum != 0')\n",
    "first_appearance = not_zero_order.groupby('itemID',as_index=False)['group_backwards'].max()#remenber backwards\n",
    "first_appearance.columns = ['itemID','first_appearance']\n",
    "\n",
    "shifting2 = pd.merge(shifting, first_appearance, on=\"itemID\",how=\"left\", validate=\"m:1\")\n",
    "\n",
    "#putting in relation with the current timestamp\n",
    "#positive means that the itemID was never sold\n",
    "#negative means that the itemID was already sold\n",
    "shifting2['first_appearance'] = shifting2['group_backwards'] - shifting2['first_appearance'] \n",
    "\n",
    "#removing dataleak\n",
    "func = lambda x : np.nan if x >= 0 else x\n",
    "shifting2['first_appearance'] = shifting2['first_appearance'].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'percentage_accum_cat_3_1', 'orderSum_4', 'percentage_accum_cat_3_3', 'orderSum_diff_1', 'first_appearance', 'orderSum_2', 'orderSum_3', 'percentage_accum_cat_3_2', 'percentage_accum_cat_3_4', 'orderSum_diff_3', 'orderSum_1', 'orderSum_diff_2', 'orderSum_diff_4'}\n"
     ]
    }
   ],
   "source": [
    "orders4_columns = set(shifting2.columns)\n",
    "print(orders4_columns - orders3_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum error\n",
    "The maximum error we could get in this dataset would be just guessing the mean of our sales from weeks 1 to 12, and that's what the cell below is computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing the mean of 'orderSum' for all items in target 90.29706562119341\n"
     ]
    }
   ],
   "source": [
    "worst_possible_prediction = shifting.loc[shifting.group_backwards < 13]['orderSum'].mean()\n",
    "prediction = np.full(shifting.loc[shifting.group_backwards == 13]['orderSum'].shape, worst_possible_prediction) # Array filled with the mean...\n",
    "target = shifting.loc[shifting.group_backwards == 13]['orderSum']\n",
    "print(\"Guessing the mean of 'orderSum' for all items in target\", mse(target, prediction) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "All my experiments will use weeks 13 to 3 as a train set, week 2 as our validation set and week 1 as a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting at test time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "#           \"objective\" : \"poisson\",\n",
    "          \"objective\" : \"l1\",\n",
    "          \"metric\" :\"rmse\",\n",
    "          \"learning_rate\" : 0.5,\n",
    "          'verbosity': 1,\n",
    "          'max_depth': 6,\n",
    "          'num_leaves': 32,\n",
    "          \"min_data_in_leaf\":3000,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(data, not_drop_columns):\n",
    "    new_features = list(set(data.columns) - orders2_columns)\n",
    "    for colum in not_drop_columns :\n",
    "        new_features.remove(colum)\n",
    "    \n",
    "    data = data.drop(columns=new_features)\n",
    "    train = data.loc[data.group_backwards >= 3]\n",
    "    train_full = data.loc[data.group_backwards >= 2]\n",
    "    val = data.loc[data.group_backwards == 2]\n",
    "    test = data.loc[data.group_backwards == 1]\n",
    "\n",
    "    weights = infos.set_index('itemID')['simulationPrice'].to_dict()\n",
    "\n",
    "    w_train = train['itemID'].map(weights)\n",
    "    w_val = val['itemID'].map(weights)\n",
    "    w_train_full = train_full['itemID'].map(weights)\n",
    "\n",
    "\n",
    "    y_train = train['orderSum']\n",
    "    y_train_full = train_full['orderSum']\n",
    "    y_val = val['orderSum']\n",
    "    y_test = test['orderSum']\n",
    "        \n",
    "    X_train = train.drop(columns=[\"orderSum\"])\n",
    "    X_train_full = train_full.drop(columns=[\"orderSum\"])\n",
    "    X_val = val.drop(columns=[\"orderSum\"])\n",
    "    X_test = test.drop(columns=[\"orderSum\"])\n",
    "\n",
    "\n",
    "    lgbtrain = lgb.Dataset(X_train, label = y_train, weight=w_train)\n",
    "    lgbtrain_full = lgb.Dataset(X_train_full, label = y_train_full, weight=w_train_full)\n",
    "    lgbvalid = lgb.Dataset(X_val, label = y_val, weight=w_val)\n",
    "\n",
    "    \n",
    "    num_round = 1000\n",
    "    model = lgb.train(params,\n",
    "                  lgbtrain,\n",
    "                  num_round,\n",
    "                  valid_sets = [lgbtrain, lgbvalid], \n",
    "                  verbose_eval=0,\n",
    "                  early_stopping_rounds=5,\n",
    "#                   fobj=objective,\n",
    "                  feval=feval,)\n",
    "    \n",
    "    index_best_score = model.best_iteration\n",
    "    \n",
    "    final_predictions = model.predict(X_test)\n",
    "    final_predictions[final_predictions < 0] = 0\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    scores.append(model.best_score['training']['feval'])#score train 1\n",
    "    scores.append(model.best_score['valid_1']['feval'])#score validation 1\n",
    "    #score test 1\n",
    "    scores.append(baseline_score(final_predictions, y_test.values, infos['simulationPrice']))\n",
    "    \n",
    "    #retraining with pair-week 2\n",
    "    \n",
    "    model2 = lgb.train(params,\n",
    "                lgbtrain_full,\n",
    "                num_boost_round = index_best_score,\n",
    "                valid_sets = [lgbtrain_full], \n",
    "                verbose_eval=0,\n",
    "                early_stopping_rounds=5,\n",
    "                feval=feval,)\n",
    "    \n",
    "    \n",
    "    final_predictions2 = model2.predict(X_test)\n",
    "    final_predictions2[final_predictions2 < 0] = 0\n",
    "    \n",
    "    scores.append(model2.best_score['training']['feval'])#score train 2\n",
    "    #score test 2\n",
    "    scores.append(baseline_score(final_predictions2, y_test.values, infos['simulationPrice']))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(get_result(shifting2,[]))#no new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'percentage_accum_cat_3', 'max_sale', 'percentage_accum_cat_3_1', 'orderSum_4', 'posM_l_group', 'orderSum_1', 'posM_m_group', 'posM_f_group', 'first_appearance', 'percentage_accum_cat_3_2', 'orderSum_2', 'orderSum_3', 'freq_day', 'max_sale_rec', 'min_sale', 'orderSum_diff_4', 'freq_group', 'percentage_accum_cat_3_3', 'orderSum_diff_1', 'min_sale_rec', 'percentage_accum_cat_3_4', 'orderSum_diff_3', 'orderSum_diff_2'}\n"
     ]
    }
   ],
   "source": [
    "print(set(shifting2.columns) - orders2_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valor  base: [0.0, 0.0, 0.0, 0.0, 0.0];\n",
      "utilizando a diferença da base para os valores abaixo\n",
      "score treino1, score validacao1, score test1, score treino2, score validacao2\n",
      "todas features,posicao do mes, first_appearance\n",
      "[357270.3263937055, 58272.07592918872, 42995.587999999996, 341099.4682347, 33248.91]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[58697.59612010716, 9862.858035534618, 9868.356, 124565.86794940231, 15997.854]\n",
      "\n",
      "frequencia, min max e min_rec max_rec\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[64207.7359539628, 14810.4919773221, 14176.787999999999, 121310.03404651879, 24534.143999999997]\n",
      "\n",
      " min_rec max_rec, ordermSum e orderSum_diff\n",
      "[37194.06799402236, 9325.247991836071, 9350.673999999999, 51771.89793430566, 8623.189999999999]\n",
      "[287543.92409093975, 52451.536004149915, 52080.12599999999, 235186.23024313443, 34563.268]\n",
      "\n",
      "ordermSum , orderSum_diff\n",
      "[287543.92409093975, 52451.536004149915, 52080.12599999999, 235186.23024313443, 34563.268]\n",
      "[149613.57214532493, 25416.067952919006, 24432.782, 184355.11032218335, 20863.668]\n",
      "\n",
      "[178363.7081048727, 20893.324071455, 19962.534, 249592.37813420294, 22865.083999999995]\n"
     ]
    }
   ],
   "source": [
    "base = get_result(shifting2,[])#no new features\n",
    "diff = lambda x: [x - b for x,b in zip(x,base)]\n",
    "\n",
    "print(f\"valor  base: {base};\")\n",
    "print(\"utilizando a diferença da base para os valores abaixo\")\n",
    "print(f\"score treino1, score validacao1, score test1, score treino2, score validacao2\")\n",
    "\n",
    "print(\"todas features,posicao do mes, first_appearance\")\n",
    "print(diff(get_result(shifting2, list(set(shifting2.columns) - orders2_columns))))#all new features\n",
    "print(diff(get_result(shifting2, ['posM_f_group', 'posM_m_group','posM_l_group'])))#position in month\n",
    "print(diff(get_result(shifting2, ['first_appearance'])))#'first_appearance'\n",
    "print(f\"\")\n",
    "\n",
    "print(\"frequencia, min max e min_rec max_rec\")\n",
    "print(diff(get_result(shifting2, ['freq_day','freq_group'])))#frequency\n",
    "print(diff(get_result(shifting2, ['min_sale','max_sale','max_sale_rec','min_sale_rec'])))#min max\n",
    "print(f\"\")\n",
    "\n",
    "print(\" min_rec max_rec, ordermSum e orderSum_diff\")\n",
    "print(diff(get_result(shifting2, ['max_sale_rec','min_sale_rec'])))#min max recent\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'orderSum_diff_1','orderSum_diff_2','orderSum_diff_3'])))\n",
    "print(f\"\")\n",
    "\n",
    "print(\"ordermSum , orderSum_diff\")\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3'])))\n",
    "print(diff(get_result(shifting2, ['orderSum_diff_1','orderSum_diff_2','orderSum_diff_3'])))\n",
    "\n",
    "print(f\"\")\n",
    "print(diff(get_result(shifting2, ['percentage_accum_cat_3_3','percentage_accum_cat_3_4','percentage_accum_cat_3_1','percentage_accum_cat_3_2',\n",
    "                            'percentage_accum_cat_3'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score treino1, score validacao1, score test1, score treino2, score validacao2\n",
      "\n",
      "posicao mes, first_appearance\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[143605.99178530579, 20531.571890294545, 17336.812000000005, 268229.11404116155, 32896.648]\n",
      "\n",
      "freq, min max e min_rec max_rec\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[102413.33003123396, 20008.875943148123, 18250.678, 164240.94596729288, 28888.906000000003]\n",
      "\n",
      "min e max, percentage_accum_cat\n",
      "[63016.17797181604, 4924.413992333408, 2117.6800000000076, 90969.07602602249, 18914.789999999994]\n",
      "[123161.35074998136, 19445.964060932405, 14948.198000000004, 146258.08160477283, 4499.356]\n"
     ]
    }
   ],
   "source": [
    "base = get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3'])\n",
    "diff = lambda x: [x - b for x,b in zip(x,base)]\n",
    "\n",
    "print(f\"score treino1, score validacao1, score test1, score treino2, score validacao2\\n\")\n",
    "\n",
    "print(\"posicao mes, first_appearance\")\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'posM_f_group', 'posM_m_group','posM_l_group'])))#position in month\n",
    "\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'first_appearance'])))#'first_appearance'\n",
    "print(\"\")\n",
    "\n",
    "print(\"freq, min max e min_rec max_rec\")\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'freq_day','freq_group'])))#frequency\n",
    "\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'min_sale','max_sale','max_sale_rec','min_sale_rec'])))#min max\n",
    "print(\"\")\n",
    "\n",
    "print(\"min e max, percentage_accum_cat\")\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'max_sale_rec','min_sale_rec'])))#min max recent\n",
    "\n",
    "\n",
    "print(diff(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'percentage_accum_cat_3_3','percentage_accum_cat_3_4','percentage_accum_cat_3_1','percentage_accum_cat_3_2',\n",
    "                            'percentage_accum_cat_3'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features min e max tem um aumento pequeno no score, pode ajudar um pouco ou ser ruido\n",
    "### o mesmo pode se aplicar a posicao do mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " soh semana anterior\n",
      "[221927.53613772988, 37969.266003286844, 37655.801999999996, 338420.19624308345, 49896.12599999999]\n",
      "[267898.7642508089, 56259.806106841555, 53490.168000000005, 264910.3982194542, 37059.882]\n",
      "\n",
      " duas anteriores\n",
      "[287543.92409093975, 52451.536004149915, 52080.12599999999, 235186.23024313443, 34563.268]\n",
      "[352349.25370877975, 67551.53769187331, 62382.77399999999, 378279.92024835345, 54696.498]\n",
      "\n",
      " tres semanas anteriores\n",
      "[287543.92409093975, 52451.536004149915, 52080.12599999999, 235186.23024313443, 34563.268]\n",
      "[287543.92409093975, 52451.536004149915, 52080.12599999999, 235186.23024313443, 34563.268]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n soh semana anterior')\n",
    "print(get_result(shifting2, ['orderSum_diff_1','orderSum_1']))\n",
    "print(get_result(shifting2, ['orderSum_1']))\n",
    "print('\\n duas anteriores')\n",
    "print(get_result(shifting2, ['orderSum_diff_1','orderSum_1','orderSum_diff_2','orderSum_2']))\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2']))\n",
    "print('\\n tres semanas anteriores')\n",
    "print(get_result(shifting2, ['orderSum_diff_1','orderSum_1','orderSum_diff_2','orderSum_2','orderSum_diff_3','orderSum_3']))\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando features que parecem impactar para modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[330224.7365670383, 54993.51602548361, 43107.009999999995, 406210.5981041312, 44837.06]\n"
     ]
    }
   ],
   "source": [
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                                  'first_appearance',\n",
    "                             'min_sale','max_sale','max_sale_rec','min_sale_rec',\n",
    "                                'percentage_accum_cat_3_3','percentage_accum_cat_3_4','percentage_accum_cat_3_1','percentage_accum_cat_3_2',\n",
    "                            'percentage_accum_cat_3']))#position in month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
