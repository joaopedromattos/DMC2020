{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from catboost import CatBoostRegressor, Pool, MetricVisualizer\n",
    "from sasaki_features import add_feature_position_month\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(\"../dora/models\")\n",
    "from utils import read_data, process_time, merge_data, promotionAggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some functions from dora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (por algum motivo eu nao consegui importar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promo_detector_fixed(orders, aggregation=True, mode=True):\n",
    "    \"\"\"\n",
    "    This function adds a \"promotion\" column at \"orders.csv\".\n",
    "    It verifies if an item of an order is being sold cheaper than it's prices \"mode\"/\"mean\". \n",
    "    Case affirmative, a '1' will be added in 'promotion' column in the line of the order.\n",
    "\n",
    "    Parameters: orders -> Orders DataFrame\n",
    "                aggregation -> Flag that mantains or not the \"salesPriceMode\" in our returned DataFrame\n",
    "                True => Return will have the column\n",
    "                mode -> Decision method flag (Default 'True'). If \"True\", the function will \n",
    "                use the 'mode' of the prices to decide if an item is being sold below it's normal price. \n",
    "                If 'False', we'll use the \"mean\" of the prices.\n",
    "                \n",
    "    Returns: our orders Dataframe with 2 new columns (\"salesPriceMode\" and \"promotion\")\n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "      \n",
    "    def agregationMode(x): return x.value_counts().index[0] if mode else 'mean'\n",
    "    \n",
    "    for i in range(13, -1, -1):\n",
    "        # Getting an itemID / salesPriceMode Dataframe\n",
    "        # salesPriceMode column will store the \n",
    "        # 'mean'/'mode' of our items\n",
    "        current_agg = orders.loc[orders.group_backwards > i].groupby(['itemID']).agg(salesPriceMode=('salesPrice', agregationMode))\n",
    "        \n",
    "        current_agg['promotion'] = 0\n",
    "        orders_copy = orders.loc[orders.group_backwards == i + 1].copy()\n",
    "        \n",
    "        current_orders_with_promotion = pd.merge(orders_copy, current_agg, how='inner', left_on='itemID', right_on='itemID')\n",
    "        \n",
    "        # For every item whose salesPrice is lower than the 'mean'/'mode',\n",
    "        # we'll attribute 1 to it's position in 'promotion' column\n",
    "        current_orders_with_promotion.loc[current_orders_with_promotion['salesPrice'] <\n",
    "                                                       current_orders_with_promotion['salesPriceMode'], 'promotion'] = 1\n",
    "        \n",
    "        new_df = pd.concat([new_df, current_orders_with_promotion])\n",
    "    \n",
    "    \n",
    "    week_13 = orders.loc[orders.group_backwards == 13].copy()\n",
    "    week_13['salesPriceMode'] = 0\n",
    "    week_13['promotion'] = 0\n",
    "    \n",
    "    new_df = pd.concat([new_df, week_13])\n",
    "    \n",
    "    if (not(aggregation)):\n",
    "        new_df.drop(\n",
    "            'salesPriceMode', axis=1, inplace=True)\n",
    "        \n",
    "    new_df.sort_values(by=['group_backwards', 'itemID'], inplace=True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_score(prediction, target, simulatedPrice):\n",
    "    prediction = prediction.astype(int)\n",
    "\n",
    "    return np.sum((prediction - np.maximum(prediction - target, 0) * 1.6)  * simulatedPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing orders and applying already made features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salesPrice', 'days', 'days_backwards', 'week_backwards', 'group_backwards', 'time', 'order', 'transactID', 'itemID'}\n"
     ]
    }
   ],
   "source": [
    "infos, items, orders = read_data(\"../main/datasets/\")\n",
    "process_time(orders)\n",
    "\n",
    "orders_columns = set(orders.columns)\n",
    "print(orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salesPrice_mean', 'category2', 'brand', 'promotion_mean', 'recommendedRetailPrice', 'category1', 'orderSum', 'category3', 'manufacturer', 'customerRating'}\n"
     ]
    }
   ],
   "source": [
    "orders2 = promo_detector_fixed(orders)\n",
    "orders2 = promotionAggregation(orders2, items) #depois arrumar promotion agregation\n",
    "\n",
    "\n",
    "orders2_columns = set(orders2.columns)\n",
    "print(orders2_columns - orders_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding zero ordemSum rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ids = set( orders['itemID'].unique() )\n",
    "all_ids = set(items['itemID'].unique())\n",
    "\n",
    "#OBS: if add_all_ids == false could cause dataleak, since you know the items in the table are sold\n",
    "#at least one time\n",
    "\n",
    "#add rows with zero values to the last timestamp (without dataleak)\n",
    "def prepareTimeStamp(data, time, add_all_ids = True):\n",
    "    #finding ids to add\n",
    "    last_row = data.query(f\"group_backwards == {time}\")\n",
    "    last_row_ids = set(last_row['itemID'].unique())\n",
    "    \n",
    "    if(not add_all_ids):\n",
    "        news_ids = list(valid_ids - last_row_ids)\n",
    "    else:\n",
    "        news_ids = list(all_ids - last_row_ids)\n",
    "    \n",
    "    #adding new rows\n",
    "    new_rows = pd.DataFrame({'itemID': news_ids })\n",
    "    new_rows['group_backwards'] = time\n",
    "    new_rows['salesPrice_mean'] = 0\n",
    "    new_rows['orderSum'] = 0\n",
    "    new_rows['promotion_mean'] = 0\n",
    "    \n",
    "    \n",
    "    return data.append(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareOrders(data, add_all_ids = True):\n",
    "    \n",
    "    for time in data['group_backwards'].unique():\n",
    "        data = prepareTimeStamp(data, time, add_all_ids)\n",
    "        \n",
    "\n",
    "    #remove overlapping column from merge with items\n",
    "    overlapping = list(set(data.columns) and set(items.columns)  )\n",
    "    overlapping.remove('itemID')\n",
    "    data.drop(overlapping, axis=1, inplace=True)\n",
    "    \n",
    "    data = pd.merge(data,items, on=['itemID'], how=\"left\", validate=\"m:1\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "orders3 = orders2.copy()\n",
    "orders3 = prepareOrders(orders3)\n",
    "orders3 = orders3.sort_values(['group_backwards', 'itemID'], ascending=[False, True], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders4 = add_feature_position_month(orders3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function without information from futures\n",
    "def apply_to_serie(data, function,extraParans={}):\n",
    "    \n",
    "    new_data = pd.DataFrame()\n",
    " \n",
    "    for time in data['group_backwards'].unique():\n",
    "        new_rows = function(data,time,**extraParans)\n",
    "        new_data = pd.concat([new_data, new_rows])\n",
    "        \n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_freq(data,time):\n",
    "    orders_aux = orders.query(f\"group_backwards >  {time}\")#remember, its backwards\n",
    "    \n",
    "    nDays = orders_aux['days'].nunique()\n",
    "    nWeek = orders_aux['week_backwards'].nunique()\n",
    "    nGroup = orders_aux['group_backwards'].nunique()\n",
    "\n",
    "    newInfo = items[['itemID']].copy()\n",
    "    newInfo[\"group_backwards\"] = time\n",
    "    \n",
    "    #how many days in average the item is sold in day/week/pair\n",
    "    newInfo['freq_day'] = orders_aux.groupby('itemID', as_index=False)['days'].nunique()/nDays\n",
    "    newInfo['freq_week'] = orders_aux.groupby('itemID', as_index=False)['week_backwards'].nunique()/nWeek\n",
    "    newInfo['freq_group'] = orders_aux.groupby('itemID', as_index=False)['group_backwards'].nunique()/nGroup\n",
    "    \n",
    "    current_time = data.query(f\"group_backwards == {time}\")\n",
    "    return pd.merge(current_time,newInfo, on=['itemID','group_backwards'], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "orders4 = apply_to_serie(orders4,  add_feature_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordenando features categoricas usando como metrica \n",
    "#vendas totais medias entre itemID de mesma categoria\n",
    "def add_feature_ord_cat(data, time, category):\n",
    "    orders_aux = data.query(f\"group_backwards > {time}\")#remember, its backwards\n",
    "\n",
    "    newInfo = orders_aux.groupby(category,as_index=False).agg({'orderSum' : ['sum'],'itemID' : ['count']})\n",
    "    newInfo[\"group_backwards\"] = time\n",
    "    \n",
    "\n",
    "    #calculando a metrica para cada item diferente da coluna\n",
    "    newInfo[f'avg_sales_{category}'] = newInfo[( 'orderSum',   'sum')] / newInfo[( 'itemID', 'count')]\n",
    "    newInfo = newInfo[[category,\"group_backwards\", f'avg_sales_{category}']]\n",
    "    newInfo.columns = [category,\"group_backwards\", f'avg_sales_{category}']\n",
    "    \n",
    "    \n",
    "    current_time = data.query(f\"group_backwards == {time}\")\n",
    "    return pd.merge(current_time,newInfo, on=[category,'group_backwards'], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "\n",
    "orders4 = apply_to_serie(orders4,  add_feature_ord_cat, extraParans={\"category\": \"category3\"})\n",
    "orders4 = apply_to_serie(orders4,  add_feature_ord_cat, extraParans={\"category\": \"brand\"})\n",
    "orders4 = apply_to_serie(orders4,  add_feature_ord_cat, extraParans={\"category\": \"manufacturer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_min_max(data,time):\n",
    "    orders_aux = data.query(f'group_backwards > {time}')\n",
    "\n",
    "    newInfo = items[['itemID']].copy()\n",
    "    newInfo['group_backwards'] = time\n",
    "    \n",
    "    #minimun and maximum sales in a pair\n",
    "    #too much zeros, trying for last 4 pairs\n",
    "    newInfo['min_sale'] = orders_aux.groupby(['itemID'])['orderSum'].min()\n",
    "    newInfo['max_sale'] = orders_aux.groupby(['itemID'])['orderSum'].max()\n",
    "\n",
    "\n",
    "    #minimun and maximum sales in a group recent\n",
    "    order_recent = data.query(f'group_backwards > {time} & group_backwards < {time+4}')\n",
    "    newInfo['min_sale_rec'] = order_recent.groupby(['itemID'])['orderSum'].min()\n",
    "    newInfo['max_sale_rec'] = order_recent.groupby(['itemID'])['orderSum'].max()\n",
    "\n",
    "    current_time = data.query(f\"group_backwards == {time}\")\n",
    "    return pd.merge(current_time,newInfo, on=['itemID','group_backwards'], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "orders4 = apply_to_serie(orders4,  add_feature_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add feature first appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding first apperance\n",
    "not_zero_order = orders4.query('orderSum != 0')\n",
    "first_appearance = not_zero_order.groupby('itemID',as_index=False)['group_backwards'].max()#remenber backwards\n",
    "first_appearance.columns = ['itemID','first_appearance']\n",
    "\n",
    "orders4 = pd.merge(orders4, first_appearance, on=\"itemID\",how=\"left\", validate=\"m:1\")\n",
    "\n",
    "#putting in relation with the current timestamp\n",
    "#positive means that the itemID was never sold\n",
    "#negative means that the itemID was already sold\n",
    "orders4['first_appearance'] = orders4['group_backwards'] - orders4['first_appearance'] \n",
    "\n",
    "#removing dataleak\n",
    "func = lambda x : np.nan if x >= 0 else x\n",
    "orders4['first_appearance'] = orders4['first_appearance'].apply(func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'posM_f_group', 'min_sale_rec', 'max_sale_rec', 'freq_week', 'posM_m_group', 'posM_l_group', 'avg_sales_brand', 'first_appearance', 'freq_group', 'max_sale', 'avg_sales_category3', 'freq_day', 'avg_sales_manufacturer', 'min_sale'}\n"
     ]
    }
   ],
   "source": [
    "orders4_columns = set(orders4.columns)\n",
    "print(orders4_columns - orders2_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74609"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#freq feature has lots of nan; just checking if there is a error\n",
    "print(len(first_appearance[first_appearance.first_appearance == 1]) * 13 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 2]) * 12 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 3]) * 11 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 4]) * 10 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 5]) * 9 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 6]) * 8 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 7]) * 7 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 8]) * 6 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 9]) * 5 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 10]) * 4 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 11]) * 3 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 12]) * 2 + \\\n",
    "len(first_appearance[first_appearance.first_appearance == 13]) * 1 + \\\n",
    "len(all_ids - valid_ids) * 13 )\n",
    "\n",
    "display(orders4.freq_day.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift\n",
    "### added the feature salesPrice_mean_ from older pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'promotion_mean_2', 'salesPrice_mean_diff_2', 'salesPrice_mean_1', 'orderSum_2', 'orderSum_1', 'salesPrice_mean_diff_1', 'orderSum_diff_2', 'orderSum_diff_1', 'promotion_mean_1', 'salesPrice_mean_2', 'promotion_mean_diff_1', 'promotion_mean_diff_2'}\n"
     ]
    }
   ],
   "source": [
    "orders5 = orders4.copy()\n",
    "\n",
    "for i in range(1, 3):\n",
    "    # Carrying the data of weeks t-1\n",
    "    orders5[f'orderSum_{i}'] = orders5.groupby('itemID')['orderSum'].shift(i)\n",
    "    orders5[f'promotion_mean_{i}'] = orders5.groupby('itemID')['promotion_mean'].shift(i)\n",
    "    orders5[f'salesPrice_mean_{i}'] = orders5.groupby('itemID')['salesPrice_mean'].shift(i)\n",
    "    \n",
    "    # Getting the difference of the orders and promotions between weeks t-1 and t-2...\n",
    "    orders5[f'orderSum_diff_{i}'] = orders5.groupby('itemID')[f'orderSum_{i}'].diff()\n",
    "    orders5[f'promotion_mean_diff_{i}'] = orders5.groupby('itemID')[f'promotion_mean_{i}'].diff()\n",
    "    orders5[f'salesPrice_mean_diff_{i}'] = orders5.groupby('itemID')[f'salesPrice_mean_{i}'].diff()\n",
    "\n",
    "orders5 =orders5.fillna(np.inf)\n",
    "\n",
    "orders5_columns = set(orders5.columns)\n",
    "print(orders5_columns - orders4_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_obj(object):\n",
    "    def __iter__(self):\n",
    "        return iter('custom')\n",
    "    \n",
    "    def get_final_error(self, error, weight):\n",
    "    \n",
    "        return error\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        approx = approxes[0]\n",
    "\n",
    "        error_sum = 0.0\n",
    "        weight_sum = 0.0\n",
    "\n",
    "        for prediction,t,w in zip(approx, target, weight):\n",
    "            \n",
    "            weight_sum += w\n",
    "            \n",
    "            error_sum += -1* (prediction - (np.maximum(prediction - t, 0) * 1.6))  * w\n",
    "\n",
    "        return error_sum, weight_sum\n",
    "    def calc_ders_range(self, approxes, targets, weights):\n",
    "        pred = np.array(approxes)\n",
    "        target = np.array(targets)\n",
    "        weight = np.array(weights)\n",
    "        \n",
    "        \n",
    "        der1 = -2 *weight* (pred - (np.maximum(pred - target, 0) * 1.6)) * (1 - (pred > target) * 1.6)\n",
    "        der2 = -2 *weight* (1 - (pred > target) * 1.6) ** 2\n",
    "\n",
    "        return list(zip(der1,der2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['group_backwards', 'itemID', 'orderSum', 'promotion_mean',\n",
       "       'salesPrice_mean', 'brand', 'manufacturer', 'customerRating',\n",
       "       'category1', 'category2', 'category3', 'recommendedRetailPrice',\n",
       "       'posM_f_group', 'posM_m_group', 'posM_l_group', 'freq_day', 'freq_week',\n",
       "       'freq_group', 'avg_sales_category3', 'avg_sales_brand',\n",
       "       'avg_sales_manufacturer', 'min_sale', 'max_sale', 'min_sale_rec',\n",
       "       'max_sale_rec', 'first_appearance', 'orderSum_1', 'promotion_mean_1',\n",
       "       'salesPrice_mean_1', 'orderSum_diff_1', 'promotion_mean_diff_1',\n",
       "       'salesPrice_mean_diff_1', 'orderSum_2', 'promotion_mean_2',\n",
       "       'salesPrice_mean_2', 'orderSum_diff_2', 'promotion_mean_diff_2',\n",
       "       'salesPrice_mean_diff_2'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orders6 = orders5.copy()\n",
    "\n",
    "display(orders6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical features\n",
    "cat_features = ['brand','manufacturer','category1','category2','category3']\n",
    "\n",
    "#passing to integer\n",
    "for f in cat_features:\n",
    "    orders6[f] = orders6[f].map(lambda x : int(x))\n",
    "    \n",
    "weight =pd.merge(orders6, infos[[\"itemID\", \"simulationPrice\"]], \n",
    "                     on=\"itemID\", validate=\"m:1\")\n",
    "weightt = weight[[\"itemID\",\"group_backwards\",\"simulationPrice\"]]\n",
    "\n",
    "params = {'iterations': 200, \n",
    "         'loss_function':'RMSE',\n",
    "         'use_best_model': True,\n",
    "         'early_stopping_rounds': 30,\n",
    "}\n",
    "\n",
    "params2= {'loss_function':custom_obj(),\n",
    "         'iterations': 200, \n",
    "         'eval_metric':custom_obj(),\n",
    "         'use_best_model': True,\n",
    "         'early_stopping_rounds': 30,\n",
    "         'subsample':1,\n",
    "         }\n",
    "\n",
    "\n",
    "params3= {'loss_function':'RMSE',\n",
    "         'iterations': 200, \n",
    "         'eval_metric':custom_obj(),\n",
    "         'early_stopping_rounds': 30,\n",
    "         'use_best_model': True,\n",
    "         }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD NOT SOLD ITEMS IF YOUR MODEL DONT PREDICT ALL\n",
    "def get_pred(modelo,test, nome):\n",
    "    \n",
    "    \n",
    "    test_pool = Pool(test.drop(columns=[\"orderSum\"]),\n",
    "                 weight= test['salesPrice_mean'],\n",
    "                 cat_features= cat_features\n",
    "    ) \n",
    "        \n",
    "    preds = modelo.predict(test_pool)\n",
    "\n",
    "    #all prediction need to be positive and integer\n",
    "    sold_items = test.copy()\n",
    "    preds = [max(x,0) for x in preds ]\n",
    "    sold_items['demandPrediction'] = preds\n",
    "    sold_items = sold_items[[\"itemID\", \"demandPrediction\"]]\n",
    "\n",
    "    sold_items[\"demandPrediction\"] = sold_items[\"demandPrediction\"].astype(np.uint8)\n",
    "\n",
    "    #to kagle csv\n",
    "    return sold_items.sort_values(['itemID'],  ignore_index=True)\n",
    "    #final.to_csv(f\"pred/{nome}.csv\", index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(data, not_drop_columns):\n",
    "    \n",
    "    new_features = list(orders5_columns - orders2_columns)\n",
    "    for colum in not_drop_columns :\n",
    "        new_features.remove(colum)\n",
    "    \n",
    "    data = data.drop(columns=new_features)\n",
    "    \n",
    "    test = data.query('group_backwards == 1')\n",
    "    val = data.query('group_backwards == 2')\n",
    "    train = data.query('group_backwards >= 3')\n",
    "\n",
    "\n",
    "    train_pool = Pool(\n",
    "        data= train.drop(columns=[\"orderSum\"]), \n",
    "        label= train['orderSum'], \n",
    "        weight= weightt.query('group_backwards >= 3') ,\n",
    "        cat_features= cat_features\n",
    "    )\n",
    "    \n",
    "    validation_pool = Pool(\n",
    "        data= val.drop(columns=[\"orderSum\"]), \n",
    "        label= val['orderSum'], \n",
    "        weight= weightt.query('group_backwards == 2'),\n",
    "        cat_features= cat_features\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    model=CatBoostRegressor(**params) \n",
    "    model.fit(train_pool,eval_set=validation_pool , verbose=False)\n",
    "    \n",
    "    #model2=CatBoostRegressor(**params2) \n",
    "    #model2.fit(train_pool,eval_set=validation_pool , verbose=False)\n",
    "    \n",
    "    model3=CatBoostRegressor(**params3) \n",
    "    model3.fit(train_pool,eval_set=validation_pool , verbose=False)\n",
    "    \n",
    "    \n",
    "    target = test['orderSum'].values\n",
    "    predct1 =get_pred(model,test, 'cat_pos1')['demandPrediction'].values\n",
    "    predct3 =get_pred(model3,test, 'cat_pos1')['demandPrediction'].values\n",
    "    \n",
    "    score1 = baseline_score(predct1, target, infos['simulationPrice'])\n",
    "    score3 = baseline_score(predct3, target, infos['simulationPrice'])\n",
    "    \n",
    "    return score1, score3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'posM_f_group', 'promotion_mean_2', 'posM_m_group', 'orderSum_2', 'orderSum_1', 'freq_group', 'promotion_mean_1', 'max_sale_rec', 'posM_l_group', 'salesPrice_mean_diff_2', 'avg_sales_brand', 'orderSum_diff_1', 'salesPrice_mean_2', 'promotion_mean_diff_1', 'min_sale', 'salesPrice_mean_1', 'salesPrice_mean_diff_1', 'orderSum_diff_2', 'first_appearance', 'max_sale', 'avg_sales_category3', 'freq_day', 'avg_sales_manufacturer', 'min_sale_rec', 'freq_week', 'promotion_mean_diff_2'}\n"
     ]
    }
   ],
   "source": [
    "print(orders5_columns - orders2_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357700.09799999953, 367224.22599999956)\n",
      "(-190105.47200000065, -1001292.8480000008)\n",
      "(609259.2739999995, -5938897.162000001)\n"
     ]
    }
   ],
   "source": [
    "print(get_result(orders6,[]))#all new features\n",
    "\n",
    "print(get_result(orders6, list(orders5_columns - orders2_columns)))#no new features\n",
    "\n",
    "print(get_result(orders6, ['posM_f_group', 'posM_m_group','posM_l_group']))#position in month\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(562592.4199999996, 380524.3699999995)\n",
      "(-238435.58400000064, 115259.5059999995)\n",
      "(-726034.5860000008, -864571.0920000008)\n",
      "(114680.34999999955, 183382.8399999995)\n",
      "(292124.8939999995, -27755.78200000053)\n"
     ]
    }
   ],
   "source": [
    "print(get_result(orders6, ['first_appearance']))#'first_appearance'\n",
    "\n",
    "print(get_result(orders6, ['freq_day','freq_group','freq_week']))#frequency\n",
    "\n",
    "print(get_result(orders6, ['avg_sales_brand','avg_sales_manufacturer','avg_sales_category3']))#avg sales\n",
    "\n",
    "print(get_result(orders6, ['min_sale','max_sale','max_sale_rec','min_sale_rec']))#min max\n",
    "\n",
    "print(get_result(orders6, ['max_sale_rec','min_sale_rec']))#min max recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356447.6979999995, 295010.98399999953)\n",
      "(-11699.508000000511, -48602.39800000058)\n",
      "(134665.21399999945, 341139.3339999995)\n"
     ]
    }
   ],
   "source": [
    "print(get_result(orders6, ['salesPrice_mean_1','salesPrice_mean_diff_1','salesPrice_mean_2','salesPrice_mean_diff_2']))\n",
    "\n",
    "print(get_result(orders6, ['promotion_mean_1','promotion_mean_2','promotion_mean_diff_1','promotion_mean_diff_2']))\n",
    "\n",
    "print(get_result(orders6, ['orderSum_1','orderSum_2','orderSum_diff_1','orderSum_diff_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327392.9499999995, -12338.390000000538)\n"
     ]
    }
   ],
   "source": [
    "print(get_result(orders6, ['salesPrice_mean_1','salesPrice_mean_diff_1','salesPrice_mean_2','salesPrice_mean_diff_2',\n",
    "                          'first_appearance','min_sale','max_sale','max_sale_rec','min_sale_rec',\n",
    "                          'orderSum_1','orderSum_2','orderSum_diff_1','orderSum_diff_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
