{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM - Accumulated Sales of Category 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "#from sasaki_features import add_feature_position_month\n",
    "sys.path.append(\"../dora/models\")\n",
    "from utils import read_data, process_time, merge_data, dataset_builder, cumulative_sale_by_category\n",
    "\n",
    "NUMBER_OF_LAGS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sasaki_features import add_feature_position_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_score(prediction, target, simulatedPrice):\n",
    "    prediction = prediction.astype(int)\n",
    "\n",
    "    return np.sum((prediction - np.maximum(prediction - target, 0) * 1.6)  * simulatedPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval(prediction, dtrain):\n",
    "    \n",
    "    prediction = prediction.astype(int)\n",
    "    target = dtrain.get_label()\n",
    "\n",
    "    simulatedPrice = dtrain.get_weight()\n",
    "    \n",
    "    return 'feval', np.sum((prediction - np.maximum(prediction - target, 0) * 1.6)  * simulatedPrice), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(predt, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    sp = dtrain.get_weight()\n",
    "    return -2 * (predt - np.maximum(predt - y, 0) * 1.6) * (1 - (predt > y) * 1.6) * sp\n",
    "\n",
    "def hessian(predt, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    sp = dtrain.get_weight() \n",
    "    return -2 * ((1 - (predt > y) * 1.6) ** 2) * sp\n",
    "\n",
    "def objective(predt, dtrain):\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our dataset\n",
    "This notebook makes this step cleaner than the previous versions. So It'll be tidier and shorter than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'days', 'salesPrice', 'transactID', 'days_backwards', 'itemID', 'order', 'time', 'group_backwards'}\n"
     ]
    }
   ],
   "source": [
    "infos, items, orders = read_data(\"../main/datasets/\")\n",
    "process_time(orders)\n",
    "\n",
    "orders_columns = set(orders.columns)\n",
    "print(orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_builder(orders, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category3', 'category2', 'customerRating', 'manufacturer', 'category1', 'orderSum', 'recommendedRetailPrice', 'brand'}\n"
     ]
    }
   ],
   "source": [
    "orders2_columns = set(df.columns)\n",
    "print(orders2_columns - orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function without information from futures\n",
    "def apply_to_serie(data, function,extraParans={}):\n",
    "    \n",
    "    new_data = pd.DataFrame()\n",
    " \n",
    "    for time in data['group_backwards'].unique():\n",
    "        new_rows = function(data,time,**extraParans)\n",
    "        new_data = pd.concat([new_data, new_rows])\n",
    "        \n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_freq(data,time):\n",
    "    \n",
    "    #a linha de baixo utiliza o orders sem row com orderm 0\n",
    "    orders_aux = orders.query(f\"group_backwards >  {time}\")#remember, its backwards\n",
    "    \n",
    "    nDays = orders_aux['days'].nunique()\n",
    "    #nWeek = orders_aux['week_backwards'].nunique()\n",
    "    nGroup = orders_aux['group_backwards'].nunique()\n",
    "\n",
    "    newInfo = items[['itemID']].copy()\n",
    "    newInfo[\"group_backwards\"] = time\n",
    "    \n",
    "    #how many days in average the item is sold in day/week/pair\n",
    "    newInfo['freq_day'] = orders_aux.groupby('itemID', as_index=False)['days'].nunique()/nDays\n",
    "    #newInfo['freq_week'] = orders_aux.groupby('itemID', as_index=False)['week_backwards'].nunique()/nWeek\n",
    "    newInfo['freq_group'] = orders_aux.groupby('itemID', as_index=False)['group_backwards'].nunique()/nGroup\n",
    "    \n",
    "    current_time = data.query(f\"group_backwards == {time}\")\n",
    "    return pd.merge(current_time,newInfo, on=['itemID','group_backwards'], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "df2 = apply_to_serie(df,  add_feature_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_min_max(data,time):\n",
    "    orders_aux = data.query(f'group_backwards > {time}')\n",
    "\n",
    "    newInfo = items[['itemID']].copy()\n",
    "    newInfo['group_backwards'] = time\n",
    "    \n",
    "    #minimun and maximum sales in a pair\n",
    "    #too much zeros, trying for last 4 pairs\n",
    "    newInfo['min_sale'] = orders_aux.groupby(['itemID'])['orderSum'].min()\n",
    "    newInfo['max_sale'] = orders_aux.groupby(['itemID'])['orderSum'].max()\n",
    "\n",
    "\n",
    "    #minimun and maximum sales in a group recent\n",
    "    order_recent = data.query(f'group_backwards > {time} & group_backwards < {time+4}')\n",
    "    newInfo['min_sale_rec'] = order_recent.groupby(['itemID'])['orderSum'].min()\n",
    "    newInfo['max_sale_rec'] = order_recent.groupby(['itemID'])['orderSum'].max()\n",
    "\n",
    "    current_time = data.query(f\"group_backwards == {time}\")\n",
    "    return pd.merge(current_time,newInfo, on=['itemID','group_backwards'], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "df2 = apply_to_serie(df2,  add_feature_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = add_feature_position_month(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage_accum_cat_3 feature...\n",
    "df3 = cumulative_sale_by_category(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_sale', 'min_sale_rec', 'posM_l_group', 'min_sale', 'freq_group', 'max_sale_rec', 'freq_day', 'percentage_accum_cat_3', 'posM_m_group', 'posM_f_group'}\n"
     ]
    }
   ],
   "source": [
    "orders3_columns = set(df3.columns)\n",
    "print(orders3_columns - orders2_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell lags and diffs our feature 'orderSum'\n",
    "shifting = df3.copy()\n",
    "\n",
    "for i in range(1, NUMBER_OF_LAGS + 1):\n",
    "    # Carrying the data of weeks t-1\n",
    "    shifting[f'orderSum_{i}'] = shifting.groupby('itemID')['orderSum'].shift(i)\n",
    "    shifting[f'percentage_accum_cat_3_{i}'] = shifting.groupby('itemID')['percentage_accum_cat_3'].shift(i)\n",
    "\n",
    "    \n",
    "    # Getting the difference of the orders and promotions between weeks t-1 and t-2...\n",
    "    shifting[f'orderSum_diff_{i}'] = shifting.groupby('itemID')[f'orderSum_{i}'].diff()\n",
    "    shifting[f'percentage_accum_cat_3_{i}'] = shifting.groupby('itemID')[f'percentage_accum_cat_3_{i}'].diff()\n",
    "    \n",
    "# LGBM Says on docs that it automatically handles zero values as NaN\n",
    "shifting.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding first apperance\n",
    "not_zero_order = shifting.query('orderSum != 0')\n",
    "first_appearance = not_zero_order.groupby('itemID',as_index=False)['group_backwards'].max()#remenber backwards\n",
    "first_appearance.columns = ['itemID','first_appearance']\n",
    "\n",
    "shifting2 = pd.merge(shifting, first_appearance, on=\"itemID\",how=\"left\", validate=\"m:1\")\n",
    "\n",
    "#putting in relation with the current timestamp\n",
    "#positive means that the itemID was never sold\n",
    "#negative means that the itemID was already sold\n",
    "shifting2['first_appearance'] = shifting2['group_backwards'] - shifting2['first_appearance'] \n",
    "\n",
    "#removing dataleak\n",
    "func = lambda x : np.nan if x >= 0 else x\n",
    "shifting2['first_appearance'] = shifting2['first_appearance'].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orderSum_3', 'orderSum_1', 'percentage_accum_cat_3_2', 'orderSum_diff_3', 'orderSum_diff_2', 'orderSum_2', 'percentage_accum_cat_3_4', 'first_appearance', 'orderSum_diff_1', 'orderSum_diff_4', 'orderSum_4', 'percentage_accum_cat_3_1', 'percentage_accum_cat_3_3'}\n"
     ]
    }
   ],
   "source": [
    "orders4_columns = set(shifting2.columns)\n",
    "print(orders4_columns - orders3_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum error\n",
    "The maximum error we could get in this dataset would be just guessing the mean of our sales from weeks 1 to 12, and that's what the cell below is computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing the mean of 'orderSum' for all items in target 90.29706562119341\n"
     ]
    }
   ],
   "source": [
    "worst_possible_prediction = shifting.loc[shifting.group_backwards < 13]['orderSum'].mean()\n",
    "prediction = np.full(shifting.loc[shifting.group_backwards == 13]['orderSum'].shape, worst_possible_prediction) # Array filled with the mean...\n",
    "target = shifting.loc[shifting.group_backwards == 13]['orderSum']\n",
    "print(\"Guessing the mean of 'orderSum' for all items in target\", mse(target, prediction) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "All my experiments will use weeks 13 to 3 as a train set, week 2 as our validation set and week 1 as a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting at test time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "#           \"objective\" : \"poisson\",\n",
    "          \"objective\" : \"l1\",\n",
    "          \"metric\" :\"rmse\",\n",
    "          \"learning_rate\" : 0.1,\n",
    "          'verbosity': 1,\n",
    "          'max_depth': 6,\n",
    "          'num_leaves': 15,\n",
    "          \"min_data_in_leaf\":2000,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(data, not_drop_columns):\n",
    "    new_features = list(set(data.columns) - orders2_columns)\n",
    "    for colum in not_drop_columns :\n",
    "        new_features.remove(colum)\n",
    "    \n",
    "    data = data.drop(columns=new_features)\n",
    "    train = data.loc[data.group_backwards >= 3]\n",
    "    val = data.loc[data.group_backwards == 2]\n",
    "    test = data.loc[data.group_backwards == 1]\n",
    "\n",
    "    weights = infos.set_index('itemID')['simulationPrice'].to_dict()\n",
    "\n",
    "    w_train = train['itemID'].map(weights)\n",
    "    w_val = val['itemID'].map(weights)\n",
    "\n",
    "\n",
    "    y_train = train['orderSum']\n",
    "    y_val = val['orderSum']\n",
    "    X_train = train.drop(columns=[\"orderSum\"])\n",
    "    X_val = val.drop(columns=[\"orderSum\"])\n",
    "\n",
    "\n",
    "    lgbtrain = lgb.Dataset(X_train, label = y_train, weight=w_train)\n",
    "    lgbvalid = lgb.Dataset(X_val, label = y_val, weight=w_val)\n",
    "\n",
    "    num_round = 1000\n",
    "    model = lgb.train(params,\n",
    "                  lgbtrain,\n",
    "                  num_round,\n",
    "                  valid_sets = [lgbtrain, lgbvalid], \n",
    "                  verbose_eval=0,\n",
    "                  early_stopping_rounds=5,\n",
    "#                   fobj=objective,\n",
    "                  feval=feval,)\n",
    "\n",
    "\n",
    "    y_test = test['orderSum']\n",
    "    X_test = test.drop(columns=[\"orderSum\"])\n",
    "    final_predictions = model.predict(X_test)\n",
    "\n",
    "    final_predictions[final_predictions < 0] = 0\n",
    "\n",
    "    return baseline_score(final_predictions, y_test.values, infos['simulationPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'posM_l_group', 'orderSum_diff_2', 'first_appearance', 'percentage_accum_cat_3_3', 'posM_f_group', 'max_sale', 'orderSum_3', 'orderSum_1', 'freq_group', 'freq_day', 'posM_m_group', 'min_sale_rec', 'min_sale', 'orderSum_diff_3', 'orderSum_2', 'percentage_accum_cat_3_4', 'orderSum_diff_1', 'orderSum_diff_4', 'orderSum_4', 'max_sale_rec', 'percentage_accum_cat_3_2', 'percentage_accum_cat_3', 'percentage_accum_cat_3_1'}\n"
     ]
    }
   ],
   "source": [
    "print(set(shifting2.columns) - orders2_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "8350.74\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "15465.743999999999\n",
      "32055.995999999996\n",
      "9855.214\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(get_result(shifting2,[]))#no new features\n",
    "\n",
    "print(get_result(shifting2, list(set(shifting2.columns) - orders2_columns)))#all new features\n",
    "\n",
    "print(get_result(shifting2, ['posM_f_group', 'posM_m_group','posM_l_group']))#position in month\n",
    "\n",
    "print(get_result(shifting2, ['first_appearance']))#'first_appearance'\n",
    "\n",
    "print(get_result(shifting2, ['freq_day','freq_group']))#frequency\n",
    "\n",
    "print(get_result(shifting2, ['min_sale','max_sale','max_sale_rec','min_sale_rec']))#min max\n",
    "\n",
    "print(get_result(shifting2, ['max_sale_rec','min_sale_rec']))#min max recent\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'orderSum_diff_1','orderSum_diff_2','orderSum_diff_3']))\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3']))\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_diff_1','orderSum_diff_2','orderSum_diff_3']))\n",
    "\n",
    "print(get_result(shifting2, ['percentage_accum_cat_3_3','percentage_accum_cat_3_4','percentage_accum_cat_3_1','percentage_accum_cat_3_2',\n",
    "                            'percentage_accum_cat_3']))\n",
    "#nenhuma outra feature ajuda sem o orderm das semanas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47221.562000000005\n",
      "15736.068\n",
      "32055.995999999996\n",
      "47109.98199999999\n",
      "38018.914\n",
      "30938.184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'posM_f_group', 'posM_m_group','posM_l_group']))#position in month\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'first_appearance']))#'first_appearance'\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'freq_day','freq_group']))#frequency\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'min_sale','max_sale','max_sale_rec','min_sale_rec']))#min max\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'max_sale_rec','min_sale_rec']))#min max recent\n",
    "\n",
    "\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3',\n",
    "                             'percentage_accum_cat_3_3','percentage_accum_cat_3_4','percentage_accum_cat_3_1','percentage_accum_cat_3_2',\n",
    "                            'percentage_accum_cat_3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " soh semana anterior\n",
      "24359.47\n",
      "17609.922\n",
      "\n",
      " duas anteriores\n",
      "20960.606\n",
      "39649.575999999994\n",
      "\n",
      " tres semanas anteriores\n",
      "15465.743999999999\n",
      "32055.995999999996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n soh semana anterior')\n",
    "print(get_result(shifting2, ['orderSum_diff_1','orderSum_1']))\n",
    "print(get_result(shifting2, ['orderSum_1']))\n",
    "print('\\n duas anteriores')\n",
    "print(get_result(shifting2, ['orderSum_diff_1','orderSum_1','orderSum_diff_2','orderSum_2']))\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2']))\n",
    "print('\\n tres semanas anteriores')\n",
    "print(get_result(shifting2, ['orderSum_diff_1','orderSum_1','orderSum_diff_2','orderSum_2','orderSum_diff_3','orderSum_3']))\n",
    "print(get_result(shifting2, ['orderSum_1','orderSum_2','orderSum_3']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
