{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Enhanced Features\n",
    "This notebook is being created after the addition of Promotion feature to the dataset and the main goal is to submit the predictions of this notebook in our private Kaggle Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0v.zip\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import read_data, process_time, merge_data, promo_detector, promotionAggregation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "\n",
    "NUMBER_OF_LAGS = 1\n",
    "\n",
    "sys.path.append(\"../../main/datasets/\")\n",
    "!ls  ../../main/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our dataset\n",
    "These steps were already seen on ```../pre-processing-features``` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checks... (10463, 3) (10463, 8) (2181955, 5)\n"
     ]
    }
   ],
   "source": [
    "infos, items, orders = read_data(\"../../main/datasets/\")\n",
    "print(\"Sanity checks...\", infos.shape, items.shape, orders.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing our time signatures, \n",
    "# adding our promotion feature \n",
    "# and aggregating our data by weeks...\n",
    "process_time(orders)\n",
    "orders = promo_detector(orders)\n",
    "df = promotionAggregation(orders, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareOrders(orders, items):\n",
    "    \"\"\"This function is responsible for adding in our 'orders' dataframe\n",
    "    the items that were not sold. THIS IS NOT MODULARIZED, THUS YOU\n",
    "    SHOULD CHANGE THE CODE TO BETTER SUIT YOUR DATASET FEATURES\n",
    "    \"\"\"\n",
    "    \n",
    "    df = orders.copy()\n",
    "    \n",
    "    # Getting the IDs that were never sold\n",
    "    not_sold_items = items[np.logical_not(\n",
    "        items.itemID.isin(sorted(orders['itemID'].unique())))]\n",
    "\n",
    "    new_rows = []\n",
    "    weeks_database = orders['group_backwards'].unique()\n",
    "\n",
    "    for idd in df['itemID'].unique():\n",
    "        orders_id = df[df.itemID == idd]\n",
    "        example = orders_id.iloc[0]\n",
    "\n",
    "        # finding weeks without itemID sales\n",
    "        weeks_id = orders_id['group_backwards'].unique()\n",
    "        weeks_without_id = np.setdiff1d(weeks_database, weeks_id)\n",
    "\n",
    "        # creating new row\n",
    "        for w in weeks_without_id:\n",
    "            new_rows.append({'itemID': idd,\n",
    "                             'group_backwards': w,\n",
    "                             'salesPrice_mean': 0,\n",
    "                             'customerRating': example['customerRating'],\n",
    "                             'category1': example['category1'],\n",
    "                             'category2': example['category2'],\n",
    "                             'category3': example['category3'],\n",
    "                             'recommendedRetailPrice': example['recommendedRetailPrice'],\n",
    "                             'orderSum': 0,\n",
    "                             'manufacturer': example['manufacturer'],\n",
    "                             'brand': example['brand'],\n",
    "                             'promotion_mean': 0\n",
    "                             })\n",
    "    #  Adding rows in every week with the IDs of the\n",
    "    # items that were never sold.\n",
    "    df = df.append(new_rows)\n",
    "    not_sold_orders = pd.DataFrame()\n",
    "    for i in range(1, 14):\n",
    "        aux = not_sold_items.copy()\n",
    "        aux['group_backwards'] = i\n",
    "        aux['salesPrice_mean'] = 0\n",
    "        aux['promotion_mean'] = 0\n",
    "        aux['orderSum'] = 0\n",
    "        not_sold_orders = pd.concat([not_sold_orders, aux], axis=0)\n",
    "    df = pd.concat([df, not_sold_orders], axis=0).sort_values(\n",
    "        ['group_backwards', 'itemID'], ascending=[False, True], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepareOrders(df, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_backwards</th>\n",
       "      <th>itemID</th>\n",
       "      <th>orderSum</th>\n",
       "      <th>promotion_mean</th>\n",
       "      <th>salesPrice_mean</th>\n",
       "      <th>brand</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>customerRating</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>recommendedRetailPrice</th>\n",
       "      <th>orderSum_1</th>\n",
       "      <th>promotion_mean_1</th>\n",
       "      <th>orderSum_diff_1</th>\n",
       "      <th>promotion_mean_diff_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.92</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.89</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.17</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.04</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136014</th>\n",
       "      <td>1</td>\n",
       "      <td>10459</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>180.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>56.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136015</th>\n",
       "      <td>1</td>\n",
       "      <td>10460</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>163.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136016</th>\n",
       "      <td>1</td>\n",
       "      <td>10461</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>128.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136017</th>\n",
       "      <td>1</td>\n",
       "      <td>10462</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>180.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>166.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136018</th>\n",
       "      <td>1</td>\n",
       "      <td>10463</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>154.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136019 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        group_backwards  itemID  orderSum  promotion_mean  salesPrice_mean  \\\n",
       "0                    13       1         0             0.0             0.00   \n",
       "1                    13       2         0             0.0             0.00   \n",
       "2                    13       3         1             0.0            14.04   \n",
       "3                    13       4         0             0.0             0.00   \n",
       "4                    13       5         2             0.0             7.84   \n",
       "...                 ...     ...       ...             ...              ...   \n",
       "136014                1   10459         0             0.0             0.00   \n",
       "136015                1   10460         0             0.0             0.00   \n",
       "136016                1   10461         0             0.0             0.00   \n",
       "136017                1   10462         0             0.0             0.00   \n",
       "136018                1   10463         0             0.0             0.00   \n",
       "\n",
       "        brand  manufacturer  customerRating  category1  category2  category3  \\\n",
       "0         0.0           1.0            4.38        1.0        1.0        1.0   \n",
       "1         0.0           2.0            3.00        1.0        2.0        1.0   \n",
       "2         0.0           3.0            5.00        1.0        3.0        1.0   \n",
       "3         0.0           2.0            4.44        1.0        2.0        1.0   \n",
       "4         0.0           2.0            2.33        1.0        1.0        1.0   \n",
       "...       ...           ...             ...        ...        ...        ...   \n",
       "136014  180.0         253.0            0.00        8.0       44.0        8.0   \n",
       "136015    0.0         253.0            0.00        8.0       44.0        8.0   \n",
       "136016    0.0         253.0            0.00        8.0       44.0        8.0   \n",
       "136017  180.0         253.0            0.00        8.0       44.0        8.0   \n",
       "136018    0.0         253.0            0.00        8.0       44.0        8.0   \n",
       "\n",
       "        recommendedRetailPrice  orderSum_1  promotion_mean_1  orderSum_diff_1  \\\n",
       "0                         8.84         inf               inf              inf   \n",
       "1                        16.92         inf               inf              inf   \n",
       "2                        15.89         inf               inf              inf   \n",
       "3                        40.17         inf               inf              inf   \n",
       "4                        17.04         inf               inf              inf   \n",
       "...                        ...         ...               ...              ...   \n",
       "136014                   56.57         0.0               0.0              0.0   \n",
       "136015                  163.81         0.0               0.0              0.0   \n",
       "136016                  128.01         0.0               0.0              0.0   \n",
       "136017                  166.97         0.0               0.0              0.0   \n",
       "136018                  154.82         0.0               0.0             -1.0   \n",
       "\n",
       "        promotion_mean_diff_1  \n",
       "0                         inf  \n",
       "1                         inf  \n",
       "2                         inf  \n",
       "3                         inf  \n",
       "4                         inf  \n",
       "...                       ...  \n",
       "136014                    0.0  \n",
       "136015                    0.0  \n",
       "136016                    0.0  \n",
       "136017                    0.0  \n",
       "136018                    0.0  \n",
       "\n",
       "[136019 rows x 16 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell lags and diffs our features 'orderSum' and \"promotion\"\n",
    "\n",
    "shifting = df.copy()\n",
    "\n",
    "for i in range(1, NUMBER_OF_LAGS + 1):\n",
    "    # Carrying the data of weeks t-1\n",
    "    shifting[f'orderSum_{i}'] = shifting.groupby('itemID')['orderSum'].shift(i)\n",
    "    shifting[f'promotion_mean_{i}'] = shifting.groupby('itemID')['promotion_mean'].shift(i)\n",
    "    \n",
    "    # Getting the difference of the orders and promotions between weeks t-1 and t-2...\n",
    "    shifting[f'orderSum_diff_{i}'] = shifting.groupby('itemID')[f'orderSum_{i}'].diff()\n",
    "    shifting[f'promotion_mean_diff_{i}'] = shifting.groupby('itemID')[f'promotion_mean_{i}'].diff()\n",
    "shifting.fillna(np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum error\n",
    "The maximum error we could get in this dataset would be just guessing the mean of our sales from weeks 1 to 12, and that's what the cell below is computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing the mean of 'orderSum' for all items in target 90.29706562119341\n"
     ]
    }
   ],
   "source": [
    "worst_possible_prediction = shifting.loc[shifting.group_backwards < 13]['orderSum'].mean()\n",
    "prediction = np.full(shifting.loc[shifting.group_backwards == 13]['orderSum'].shape, worst_possible_prediction) # Array filled with the mean...\n",
    "target = shifting.loc[shifting.group_backwards == 13]['orderSum']\n",
    "print(\"Guessing the mean of 'orderSum' for all items in target\", mse(target, prediction) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "All my experiments will use weeks 13 to 3 as a train set, week 2 as our validation set and week 1 as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shifting.loc[shifting.group_backwards >= 3]\n",
    "val = shifting.loc[shifting.group_backwards == 2]\n",
    "test = shifting.loc[shifting.group_backwards == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I recommend to the other members of the team keeping the\n",
    "# datatypes of our datasets as Pandas DataFrames instead of Numpy,\n",
    "# since It will easier to use Boosting Analysis frameworks\n",
    "y_train = train['orderSum']\n",
    "y_val = val['orderSum']\n",
    "X_train = train.drop(columns=[\"orderSum\"])\n",
    "X_val = val.drop(columns=[\"orderSum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:104.32712\tval-rmse:110.88815\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 5 rounds.\n",
      "[1]\ttrain-rmse:104.01231\tval-rmse:110.49053\n",
      "[2]\ttrain-rmse:103.71067\tval-rmse:110.09895\n",
      "[3]\ttrain-rmse:103.39635\tval-rmse:109.67645\n",
      "[4]\ttrain-rmse:103.09214\tval-rmse:109.25549\n",
      "[5]\ttrain-rmse:102.80353\tval-rmse:108.88404\n",
      "[6]\ttrain-rmse:102.50948\tval-rmse:108.48144\n",
      "[7]\ttrain-rmse:102.21883\tval-rmse:108.08079\n",
      "[8]\ttrain-rmse:101.93532\tval-rmse:107.72778\n",
      "[9]\ttrain-rmse:101.65281\tval-rmse:107.34274\n",
      "[10]\ttrain-rmse:101.38764\tval-rmse:107.00610\n",
      "[11]\ttrain-rmse:101.11524\tval-rmse:106.64477\n",
      "[12]\ttrain-rmse:100.84586\tval-rmse:106.28210\n",
      "[13]\ttrain-rmse:100.58027\tval-rmse:105.93525\n",
      "[14]\ttrain-rmse:100.32457\tval-rmse:105.62874\n",
      "[15]\ttrain-rmse:100.06397\tval-rmse:105.27467\n",
      "[16]\ttrain-rmse:99.82569\tval-rmse:104.99045\n",
      "[17]\ttrain-rmse:99.57445\tval-rmse:104.69459\n",
      "[18]\ttrain-rmse:99.34520\tval-rmse:104.41607\n",
      "[19]\ttrain-rmse:99.09758\tval-rmse:104.09394\n",
      "[20]\ttrain-rmse:98.87563\tval-rmse:103.82818\n",
      "[21]\ttrain-rmse:98.62928\tval-rmse:103.52028\n",
      "[22]\ttrain-rmse:98.39189\tval-rmse:103.21655\n",
      "[23]\ttrain-rmse:98.18176\tval-rmse:102.96318\n",
      "[24]\ttrain-rmse:97.95976\tval-rmse:102.70943\n",
      "[25]\ttrain-rmse:97.74484\tval-rmse:102.46574\n",
      "[26]\ttrain-rmse:97.48901\tval-rmse:102.18810\n",
      "[27]\ttrain-rmse:97.23556\tval-rmse:101.90913\n",
      "[28]\ttrain-rmse:96.99120\tval-rmse:101.60825\n",
      "[29]\ttrain-rmse:96.76060\tval-rmse:101.31694\n",
      "[30]\ttrain-rmse:96.52322\tval-rmse:101.04881\n",
      "[31]\ttrain-rmse:96.28994\tval-rmse:100.78004\n",
      "[32]\ttrain-rmse:96.05939\tval-rmse:100.50170\n",
      "[33]\ttrain-rmse:95.83581\tval-rmse:100.23198\n",
      "[34]\ttrain-rmse:95.61501\tval-rmse:99.97600\n",
      "[35]\ttrain-rmse:95.40138\tval-rmse:99.71489\n",
      "[36]\ttrain-rmse:95.20064\tval-rmse:99.49255\n",
      "[37]\ttrain-rmse:94.98442\tval-rmse:99.24450\n",
      "[38]\ttrain-rmse:94.79058\tval-rmse:99.02770\n",
      "[39]\ttrain-rmse:94.58372\tval-rmse:98.79476\n",
      "[40]\ttrain-rmse:94.40642\tval-rmse:98.59300\n",
      "[41]\ttrain-rmse:94.20625\tval-rmse:98.36694\n",
      "[42]\ttrain-rmse:93.99107\tval-rmse:98.13234\n",
      "[43]\ttrain-rmse:93.80789\tval-rmse:97.93604\n",
      "[44]\ttrain-rmse:93.62759\tval-rmse:97.72234\n",
      "[45]\ttrain-rmse:93.42715\tval-rmse:97.50166\n",
      "[46]\ttrain-rmse:93.23701\tval-rmse:97.29116\n",
      "[47]\ttrain-rmse:93.05745\tval-rmse:97.13927\n",
      "[48]\ttrain-rmse:92.87359\tval-rmse:96.93330\n",
      "[49]\ttrain-rmse:92.72307\tval-rmse:96.78687\n",
      "[50]\ttrain-rmse:92.54021\tval-rmse:96.57063\n",
      "[51]\ttrain-rmse:92.35651\tval-rmse:96.36035\n",
      "[52]\ttrain-rmse:92.18436\tval-rmse:96.16816\n",
      "[53]\ttrain-rmse:92.04509\tval-rmse:96.01625\n",
      "[54]\ttrain-rmse:91.86802\tval-rmse:95.81771\n",
      "[55]\ttrain-rmse:91.69751\tval-rmse:95.62567\n",
      "[56]\ttrain-rmse:91.53092\tval-rmse:95.45222\n",
      "[57]\ttrain-rmse:91.39763\tval-rmse:95.31578\n",
      "[58]\ttrain-rmse:91.22675\tval-rmse:95.13702\n",
      "[59]\ttrain-rmse:91.07677\tval-rmse:94.97501\n",
      "[60]\ttrain-rmse:90.92246\tval-rmse:94.81346\n",
      "[61]\ttrain-rmse:90.77563\tval-rmse:94.65620\n",
      "[62]\ttrain-rmse:90.61716\tval-rmse:94.50765\n",
      "[63]\ttrain-rmse:90.46510\tval-rmse:94.36054\n",
      "[64]\ttrain-rmse:90.30848\tval-rmse:94.19171\n",
      "[65]\ttrain-rmse:90.17004\tval-rmse:94.04522\n",
      "[66]\ttrain-rmse:90.02003\tval-rmse:93.88728\n",
      "[67]\ttrain-rmse:89.87651\tval-rmse:93.74847\n",
      "[68]\ttrain-rmse:89.74574\tval-rmse:93.61900\n",
      "[69]\ttrain-rmse:89.60137\tval-rmse:93.49173\n",
      "[70]\ttrain-rmse:89.45986\tval-rmse:93.34521\n",
      "[71]\ttrain-rmse:89.33384\tval-rmse:93.22504\n",
      "[72]\ttrain-rmse:89.19550\tval-rmse:93.09484\n",
      "[73]\ttrain-rmse:89.05962\tval-rmse:92.95634\n",
      "[74]\ttrain-rmse:88.93836\tval-rmse:92.84868\n",
      "[75]\ttrain-rmse:88.80396\tval-rmse:92.72922\n",
      "[76]\ttrain-rmse:88.65328\tval-rmse:92.60145\n",
      "[77]\ttrain-rmse:88.55206\tval-rmse:92.50750\n",
      "[78]\ttrain-rmse:88.41018\tval-rmse:92.38662\n",
      "[79]\ttrain-rmse:88.28458\tval-rmse:92.26865\n",
      "[80]\ttrain-rmse:88.14127\tval-rmse:92.15463\n",
      "[81]\ttrain-rmse:88.02014\tval-rmse:92.07149\n",
      "[82]\ttrain-rmse:87.89821\tval-rmse:91.99340\n",
      "[83]\ttrain-rmse:87.77469\tval-rmse:91.88998\n",
      "[84]\ttrain-rmse:87.65629\tval-rmse:91.81692\n",
      "[85]\ttrain-rmse:87.52808\tval-rmse:91.71826\n",
      "[86]\ttrain-rmse:87.39768\tval-rmse:91.61690\n",
      "[87]\ttrain-rmse:87.28313\tval-rmse:91.54979\n",
      "[88]\ttrain-rmse:87.19455\tval-rmse:91.47842\n",
      "[89]\ttrain-rmse:87.07121\tval-rmse:91.37412\n",
      "[90]\ttrain-rmse:86.94795\tval-rmse:91.28503\n",
      "[91]\ttrain-rmse:86.82177\tval-rmse:91.19114\n",
      "[92]\ttrain-rmse:86.70219\tval-rmse:91.10691\n",
      "[93]\ttrain-rmse:86.59889\tval-rmse:91.00436\n",
      "[94]\ttrain-rmse:86.48149\tval-rmse:90.93520\n",
      "[95]\ttrain-rmse:86.36756\tval-rmse:90.86966\n",
      "[96]\ttrain-rmse:86.25520\tval-rmse:90.80740\n",
      "[97]\ttrain-rmse:86.14344\tval-rmse:90.74606\n",
      "[98]\ttrain-rmse:86.03526\tval-rmse:90.68968\n",
      "[99]\ttrain-rmse:85.92692\tval-rmse:90.63032\n",
      "[100]\ttrain-rmse:85.79943\tval-rmse:90.52938\n",
      "[101]\ttrain-rmse:85.67498\tval-rmse:90.43486\n",
      "[102]\ttrain-rmse:85.55122\tval-rmse:90.33796\n",
      "[103]\ttrain-rmse:85.44559\tval-rmse:90.28738\n",
      "[104]\ttrain-rmse:85.32858\tval-rmse:90.18471\n",
      "[105]\ttrain-rmse:85.21010\tval-rmse:90.09435\n",
      "[106]\ttrain-rmse:85.09926\tval-rmse:89.99694\n",
      "[107]\ttrain-rmse:84.99789\tval-rmse:89.95971\n",
      "[108]\ttrain-rmse:84.88273\tval-rmse:89.87177\n",
      "[109]\ttrain-rmse:84.77555\tval-rmse:89.78978\n",
      "[110]\ttrain-rmse:84.66515\tval-rmse:89.71445\n",
      "[111]\ttrain-rmse:84.56212\tval-rmse:89.63791\n",
      "[112]\ttrain-rmse:84.46983\tval-rmse:89.60338\n",
      "[113]\ttrain-rmse:84.37923\tval-rmse:89.57240\n",
      "[114]\ttrain-rmse:84.27837\tval-rmse:89.50197\n",
      "[115]\ttrain-rmse:84.18715\tval-rmse:89.47424\n",
      "[116]\ttrain-rmse:84.08981\tval-rmse:89.39738\n",
      "[117]\ttrain-rmse:84.00254\tval-rmse:89.35908\n",
      "[118]\ttrain-rmse:83.90080\tval-rmse:89.29726\n",
      "[119]\ttrain-rmse:83.81319\tval-rmse:89.25996\n",
      "[120]\ttrain-rmse:83.71986\tval-rmse:89.20623\n",
      "[121]\ttrain-rmse:83.62307\tval-rmse:89.14105\n",
      "[122]\ttrain-rmse:83.52950\tval-rmse:89.07265\n",
      "[123]\ttrain-rmse:83.43259\tval-rmse:89.01939\n",
      "[124]\ttrain-rmse:83.34567\tval-rmse:89.00060\n",
      "[125]\ttrain-rmse:83.24683\tval-rmse:88.95732\n",
      "[126]\ttrain-rmse:83.15398\tval-rmse:88.90564\n",
      "[127]\ttrain-rmse:83.05488\tval-rmse:88.85630\n",
      "[128]\ttrain-rmse:82.96358\tval-rmse:88.80568\n",
      "[129]\ttrain-rmse:82.89153\tval-rmse:88.78129\n",
      "[130]\ttrain-rmse:82.80496\tval-rmse:88.73852\n",
      "[131]\ttrain-rmse:82.73435\tval-rmse:88.71557\n",
      "[132]\ttrain-rmse:82.64437\tval-rmse:88.68112\n",
      "[133]\ttrain-rmse:82.57547\tval-rmse:88.65940\n",
      "[134]\ttrain-rmse:82.48900\tval-rmse:88.62331\n",
      "[135]\ttrain-rmse:82.39330\tval-rmse:88.57786\n",
      "[136]\ttrain-rmse:82.32745\tval-rmse:88.55926\n",
      "[137]\ttrain-rmse:82.23498\tval-rmse:88.51456\n",
      "[138]\ttrain-rmse:82.17055\tval-rmse:88.49672\n",
      "[139]\ttrain-rmse:82.07747\tval-rmse:88.44118\n",
      "[140]\ttrain-rmse:81.98888\tval-rmse:88.39568\n",
      "[141]\ttrain-rmse:81.93130\tval-rmse:88.38030\n",
      "[142]\ttrain-rmse:81.87075\tval-rmse:88.33477\n",
      "[143]\ttrain-rmse:81.77753\tval-rmse:88.28622\n",
      "[144]\ttrain-rmse:81.70224\tval-rmse:88.25116\n",
      "[145]\ttrain-rmse:81.62830\tval-rmse:88.21753\n",
      "[146]\ttrain-rmse:81.55512\tval-rmse:88.18680\n",
      "[147]\ttrain-rmse:81.48704\tval-rmse:88.16430\n",
      "[148]\ttrain-rmse:81.42971\tval-rmse:88.12282\n",
      "[149]\ttrain-rmse:81.35942\tval-rmse:88.09695\n",
      "[150]\ttrain-rmse:81.28934\tval-rmse:88.06760\n",
      "[151]\ttrain-rmse:81.23341\tval-rmse:88.02858\n",
      "[152]\ttrain-rmse:81.16836\tval-rmse:88.01302\n",
      "[153]\ttrain-rmse:81.11428\tval-rmse:87.97530\n",
      "[154]\ttrain-rmse:81.06050\tval-rmse:87.93872\n",
      "[155]\ttrain-rmse:80.99738\tval-rmse:87.92512\n",
      "[156]\ttrain-rmse:80.93609\tval-rmse:87.88911\n",
      "[157]\ttrain-rmse:80.85414\tval-rmse:87.86378\n",
      "[158]\ttrain-rmse:80.79155\tval-rmse:87.84181\n",
      "[159]\ttrain-rmse:80.70650\tval-rmse:87.82365\n",
      "[160]\ttrain-rmse:80.62373\tval-rmse:87.81110\n",
      "[161]\ttrain-rmse:80.53359\tval-rmse:87.78149\n",
      "[162]\ttrain-rmse:80.45264\tval-rmse:87.76951\n",
      "[163]\ttrain-rmse:80.38966\tval-rmse:87.71934\n",
      "[164]\ttrain-rmse:80.30135\tval-rmse:87.69012\n",
      "[165]\ttrain-rmse:80.21505\tval-rmse:87.66460\n",
      "[166]\ttrain-rmse:80.14941\tval-rmse:87.65402\n",
      "[167]\ttrain-rmse:80.06508\tval-rmse:87.62882\n",
      "[168]\ttrain-rmse:79.98114\tval-rmse:87.60281\n",
      "[169]\ttrain-rmse:79.90532\tval-rmse:87.60092\n",
      "[170]\ttrain-rmse:79.82996\tval-rmse:87.59863\n",
      "[171]\ttrain-rmse:79.77115\tval-rmse:87.55263\n",
      "[172]\ttrain-rmse:79.69070\tval-rmse:87.53134\n",
      "[173]\ttrain-rmse:79.61229\tval-rmse:87.50635\n",
      "[174]\ttrain-rmse:79.54823\tval-rmse:87.48934\n",
      "[175]\ttrain-rmse:79.47289\tval-rmse:87.49083\n",
      "[176]\ttrain-rmse:79.39625\tval-rmse:87.46903\n",
      "[177]\ttrain-rmse:79.31409\tval-rmse:87.46740\n",
      "[178]\ttrain-rmse:79.23678\tval-rmse:87.44900\n",
      "[179]\ttrain-rmse:79.17641\tval-rmse:87.43148\n",
      "[180]\ttrain-rmse:79.09692\tval-rmse:87.42542\n",
      "[181]\ttrain-rmse:79.02265\tval-rmse:87.39466\n",
      "[182]\ttrain-rmse:78.97018\tval-rmse:87.35445\n",
      "[183]\ttrain-rmse:78.88689\tval-rmse:87.33654\n",
      "[184]\ttrain-rmse:78.82159\tval-rmse:87.32833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185]\ttrain-rmse:78.75133\tval-rmse:87.31165\n",
      "[186]\ttrain-rmse:78.70046\tval-rmse:87.30790\n",
      "[187]\ttrain-rmse:78.62048\tval-rmse:87.28355\n",
      "[188]\ttrain-rmse:78.56756\tval-rmse:87.28603\n",
      "[189]\ttrain-rmse:78.48887\tval-rmse:87.26333\n",
      "[190]\ttrain-rmse:78.43616\tval-rmse:87.25157\n",
      "[191]\ttrain-rmse:78.38422\tval-rmse:87.25124\n",
      "[192]\ttrain-rmse:78.32304\tval-rmse:87.24373\n",
      "[193]\ttrain-rmse:78.25006\tval-rmse:87.22503\n",
      "[194]\ttrain-rmse:78.19811\tval-rmse:87.22256\n",
      "[195]\ttrain-rmse:78.14976\tval-rmse:87.22456\n",
      "[196]\ttrain-rmse:78.08677\tval-rmse:87.21282\n",
      "[197]\ttrain-rmse:78.03642\tval-rmse:87.20534\n",
      "[198]\ttrain-rmse:77.97231\tval-rmse:87.20055\n",
      "[199]\ttrain-rmse:77.92088\tval-rmse:87.19482\n",
      "[200]\ttrain-rmse:77.87418\tval-rmse:87.19683\n",
      "[201]\ttrain-rmse:77.82885\tval-rmse:87.20520\n",
      "[202]\ttrain-rmse:77.77353\tval-rmse:87.17487\n",
      "[203]\ttrain-rmse:77.71316\tval-rmse:87.16915\n",
      "[204]\ttrain-rmse:77.66768\tval-rmse:87.17066\n",
      "[205]\ttrain-rmse:77.61461\tval-rmse:87.17286\n",
      "[206]\ttrain-rmse:77.56091\tval-rmse:87.14408\n",
      "[207]\ttrain-rmse:77.51360\tval-rmse:87.14276\n",
      "[208]\ttrain-rmse:77.45364\tval-rmse:87.13826\n",
      "[209]\ttrain-rmse:77.40131\tval-rmse:87.11194\n",
      "[210]\ttrain-rmse:77.34364\tval-rmse:87.10974\n",
      "[211]\ttrain-rmse:77.28669\tval-rmse:87.10648\n",
      "[212]\ttrain-rmse:77.22955\tval-rmse:87.10465\n",
      "[213]\ttrain-rmse:77.17451\tval-rmse:87.08189\n",
      "[214]\ttrain-rmse:77.12652\tval-rmse:87.08480\n",
      "[215]\ttrain-rmse:77.07836\tval-rmse:87.06242\n",
      "[216]\ttrain-rmse:77.03018\tval-rmse:87.06911\n",
      "[217]\ttrain-rmse:76.97691\tval-rmse:87.05212\n",
      "[218]\ttrain-rmse:76.92853\tval-rmse:87.05866\n",
      "[219]\ttrain-rmse:76.87593\tval-rmse:87.03818\n",
      "[220]\ttrain-rmse:76.83011\tval-rmse:87.04476\n",
      "[221]\ttrain-rmse:76.77866\tval-rmse:87.02584\n",
      "[222]\ttrain-rmse:76.72821\tval-rmse:87.01748\n",
      "[223]\ttrain-rmse:76.68355\tval-rmse:87.00323\n",
      "[224]\ttrain-rmse:76.63289\tval-rmse:86.99788\n",
      "[225]\ttrain-rmse:76.58332\tval-rmse:86.97944\n",
      "[226]\ttrain-rmse:76.53421\tval-rmse:86.96771\n",
      "[227]\ttrain-rmse:76.48483\tval-rmse:86.96864\n",
      "[228]\ttrain-rmse:76.43691\tval-rmse:86.95180\n",
      "[229]\ttrain-rmse:76.39554\tval-rmse:86.95300\n",
      "[230]\ttrain-rmse:76.35500\tval-rmse:86.95270\n",
      "[231]\ttrain-rmse:76.28979\tval-rmse:86.95033\n",
      "[232]\ttrain-rmse:76.23547\tval-rmse:86.92535\n",
      "[233]\ttrain-rmse:76.19121\tval-rmse:86.91911\n",
      "[234]\ttrain-rmse:76.14284\tval-rmse:86.90676\n",
      "[235]\ttrain-rmse:76.10465\tval-rmse:86.89612\n",
      "[236]\ttrain-rmse:76.06130\tval-rmse:86.89060\n",
      "[237]\ttrain-rmse:75.99852\tval-rmse:86.88876\n",
      "[238]\ttrain-rmse:75.95110\tval-rmse:86.87624\n",
      "[239]\ttrain-rmse:75.90843\tval-rmse:86.86759\n",
      "[240]\ttrain-rmse:75.86036\tval-rmse:86.87205\n",
      "[241]\ttrain-rmse:75.81944\tval-rmse:86.86694\n",
      "[242]\ttrain-rmse:75.77341\tval-rmse:86.85622\n",
      "[243]\ttrain-rmse:75.73318\tval-rmse:86.84714\n",
      "[244]\ttrain-rmse:75.68339\tval-rmse:86.84216\n",
      "[245]\ttrain-rmse:75.63841\tval-rmse:86.83559\n",
      "[246]\ttrain-rmse:75.59080\tval-rmse:86.83511\n",
      "[247]\ttrain-rmse:75.53783\tval-rmse:86.84028\n",
      "[248]\ttrain-rmse:75.48518\tval-rmse:86.81870\n",
      "[249]\ttrain-rmse:75.44496\tval-rmse:86.80783\n",
      "[250]\ttrain-rmse:75.40389\tval-rmse:86.80385\n",
      "[251]\ttrain-rmse:75.35721\tval-rmse:86.80104\n",
      "[252]\ttrain-rmse:75.32545\tval-rmse:86.79000\n",
      "[253]\ttrain-rmse:75.28545\tval-rmse:86.78602\n",
      "[254]\ttrain-rmse:75.25661\tval-rmse:86.76793\n",
      "[255]\ttrain-rmse:75.21742\tval-rmse:86.76149\n",
      "[256]\ttrain-rmse:75.16121\tval-rmse:86.75906\n",
      "[257]\ttrain-rmse:75.07480\tval-rmse:86.73534\n",
      "[258]\ttrain-rmse:74.98976\tval-rmse:86.71387\n",
      "[259]\ttrain-rmse:74.90688\tval-rmse:86.69174\n",
      "[260]\ttrain-rmse:74.82604\tval-rmse:86.66818\n",
      "[261]\ttrain-rmse:74.74979\tval-rmse:86.64571\n",
      "[262]\ttrain-rmse:74.67384\tval-rmse:86.62663\n",
      "[263]\ttrain-rmse:74.59154\tval-rmse:86.61184\n",
      "[264]\ttrain-rmse:74.50874\tval-rmse:86.59786\n",
      "[265]\ttrain-rmse:74.43845\tval-rmse:86.57746\n",
      "[266]\ttrain-rmse:74.35839\tval-rmse:86.56527\n",
      "[267]\ttrain-rmse:74.27884\tval-rmse:86.55785\n",
      "[268]\ttrain-rmse:74.20993\tval-rmse:86.53975\n",
      "[269]\ttrain-rmse:74.13240\tval-rmse:86.52686\n",
      "[270]\ttrain-rmse:74.06663\tval-rmse:86.51634\n",
      "[271]\ttrain-rmse:73.99119\tval-rmse:86.50923\n",
      "[272]\ttrain-rmse:73.91690\tval-rmse:86.50114\n",
      "[273]\ttrain-rmse:73.85554\tval-rmse:86.49163\n",
      "[274]\ttrain-rmse:73.77983\tval-rmse:86.49120\n",
      "[275]\ttrain-rmse:73.71744\tval-rmse:86.48265\n",
      "[276]\ttrain-rmse:73.64539\tval-rmse:86.47502\n",
      "[277]\ttrain-rmse:73.57770\tval-rmse:86.46648\n",
      "[278]\ttrain-rmse:73.50778\tval-rmse:86.45618\n",
      "[279]\ttrain-rmse:73.43986\tval-rmse:86.44247\n",
      "[280]\ttrain-rmse:73.37973\tval-rmse:86.44231\n",
      "[281]\ttrain-rmse:73.31203\tval-rmse:86.43421\n",
      "[282]\ttrain-rmse:73.24264\tval-rmse:86.42996\n",
      "[283]\ttrain-rmse:73.18050\tval-rmse:86.42171\n",
      "[284]\ttrain-rmse:73.11334\tval-rmse:86.41450\n",
      "[285]\ttrain-rmse:73.05277\tval-rmse:86.41094\n",
      "[286]\ttrain-rmse:72.98830\tval-rmse:86.40997\n",
      "[287]\ttrain-rmse:72.92680\tval-rmse:86.40403\n",
      "[288]\ttrain-rmse:72.86077\tval-rmse:86.40340\n",
      "[289]\ttrain-rmse:72.79574\tval-rmse:86.40170\n",
      "[290]\ttrain-rmse:72.73989\tval-rmse:86.40158\n",
      "[291]\ttrain-rmse:72.67224\tval-rmse:86.40224\n",
      "[292]\ttrain-rmse:72.61066\tval-rmse:86.40947\n",
      "[293]\ttrain-rmse:72.55009\tval-rmse:86.41313\n",
      "[294]\ttrain-rmse:72.49050\tval-rmse:86.42297\n",
      "[295]\ttrain-rmse:72.43423\tval-rmse:86.43546\n",
      "Stopping. Best iteration:\n",
      "[290]\ttrain-rmse:72.73989\tval-rmse:86.40158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train, missing=np.inf)\n",
    "dval = xgb.DMatrix(X_val, y_val, missing=np.inf)\n",
    "\n",
    "param = {'max_depth':8, 'eta':0.01, 'objective':'reg:squarederror' }\n",
    "num_round = 1000\n",
    "bst = xgb.train(param, dtrain,\n",
    "                num_round, early_stopping_rounds = 5,\n",
    "                evals = [(dtrain, 'train'), (dval, 'val')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's rmse: 83.0187\tvalid_1's rmse: 91.6425\n",
      "[40]\ttraining's rmse: 77.7523\tvalid_1's rmse: 91.787\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's rmse: 81.5195\tvalid_1's rmse: 91.1929\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "          \"objective\" : \"poisson\",\n",
    "          \"metric\" :\"rmse\",\n",
    "          \"learning_rate\" : 0.1,\n",
    "          'verbosity': 2,\n",
    "          'max_depth': 8\n",
    "         }\n",
    "\n",
    "lgbtrain = lgb.Dataset(X_train, label = y_train)\n",
    "lgbvalid = lgb.Dataset(X_val, label = y_val)\n",
    "\n",
    "num_round = 1000\n",
    "model = lgb.train(params, lgbtrain, num_round, valid_sets = [lgbtrain, lgbvalid], \n",
    "                  verbose_eval=20, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting at test time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['orderSum']\n",
    "X_test = xgb.DMatrix(test.drop(columns=[\"orderSum\"]))\n",
    "final_predictions = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating our Kaggle CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.Series(0, index=np.arange(1, len(items)+1))\n",
    "final[items.itemID] = np.rint(final_predictions)\n",
    "\n",
    "final.to_csv(\"kaggle_df.csv\", header=[\"demandPrediction\"],\n",
    "            index_label=\"itemID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving our model in disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%d-%m-%Y-%Hh%Mm%Ss\")\n",
    "modelName = 'xgb-' + now\n",
    "bst.save_model(modelName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
