{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Baseline\n",
    "This notebook is being created after the addition of Promotion feature to the dataset and the main goal is to submit the predictions of this notebook in our private Kaggle Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0v.zip\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import read_data, process_time, merge_data, promo_detector, promotionAggregation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "from catboost import CatBoost, CatBoostRegressor, Pool, cv\n",
    "\n",
    "NUMBER_OF_LAGS = 4\n",
    "\n",
    "sys.path.append(\"../../main/datasets/\")\n",
    "!ls  ../../main/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our dataset\n",
    "These steps were already seen on ```../pre-processing-features``` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checks... (10463, 3) (10463, 8) (2181955, 5)\n"
     ]
    }
   ],
   "source": [
    "infos, items, orders = read_data(\"../../main/datasets/\")\n",
    "print(\"Sanity checks...\", infos.shape, items.shape, orders.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing our time signatures, \n",
    "# adding our promotion feature \n",
    "# and aggregating our data by weeks...\n",
    "process_time(orders)\n",
    "orders = promo_detector(orders)\n",
    "df = promotionAggregation(orders, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareOrders(orders, items):\n",
    "    \"\"\"This function is responsible for adding in our 'orders' dataframe\n",
    "    the items that were not sold. THIS IS NOT MODULARIZED, THUS YOU\n",
    "    SHOULD CHANGE THE CODE TO BETTER SUIT YOUR DATASET FEATURES\n",
    "    \"\"\"\n",
    "    \n",
    "    df = orders.copy()\n",
    "    \n",
    "    # Getting the IDs that were never sold\n",
    "    not_sold_items = items[np.logical_not(\n",
    "        items.itemID.isin(sorted(orders['itemID'].unique())))]\n",
    "\n",
    "    new_rows = []\n",
    "    weeks_database = orders['group_backwards'].unique()\n",
    "\n",
    "    for idd in df['itemID'].unique():\n",
    "        orders_id = df[df.itemID == idd]\n",
    "        example = orders_id.iloc[0]\n",
    "\n",
    "        # finding weeks without itemID sales\n",
    "        weeks_id = orders_id['group_backwards'].unique()\n",
    "        weeks_without_id = np.setdiff1d(weeks_database, weeks_id)\n",
    "\n",
    "        # creating new row\n",
    "        for w in weeks_without_id:\n",
    "            new_rows.append({'itemID': idd,\n",
    "                             'group_backwards': w,\n",
    "                             'salesPrice_mean': 0,\n",
    "                             'customerRating': example['customerRating'],\n",
    "                             'category1': example['category1'],\n",
    "                             'category2': example['category2'],\n",
    "                             'category3': example['category3'],\n",
    "                             'recommendedRetailPrice': example['recommendedRetailPrice'],\n",
    "                             'orderSum': 0,\n",
    "                             'manufacturer': example['manufacturer'],\n",
    "                             'brand': example['brand'],\n",
    "                             'promotion_mean': 0\n",
    "                             })\n",
    "    #  Adding rows in every week with the IDs of the\n",
    "    # items that were never sold.\n",
    "    df = df.append(new_rows)\n",
    "    not_sold_orders = pd.DataFrame()\n",
    "    for i in range(1, 14):\n",
    "        aux = not_sold_items.copy()\n",
    "        aux['group_backwards'] = i\n",
    "        aux['salesPrice_mean'] = 0\n",
    "        aux['promotion_mean'] = 0\n",
    "        aux['orderSum'] = 0\n",
    "        not_sold_orders = pd.concat([not_sold_orders, aux], axis=0)\n",
    "    df = pd.concat([df, not_sold_orders], axis=0).sort_values(\n",
    "        ['group_backwards', 'itemID'], ascending=[False, True], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepareOrders(df, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell lags and diffs our features 'orderSum' and \"promotion\"\n",
    "\n",
    "shifting = df.copy()\n",
    "\n",
    "for i in range(1, NUMBER_OF_LAGS + 1):\n",
    "    # Carrying the data of weeks t-1\n",
    "    shifting[f'orderSum_{i}'] = shifting.groupby('itemID')['orderSum'].shift(i)\n",
    "    shifting[f'promotion_mean_{i}'] = shifting.groupby('itemID')['promotion_mean'].shift(i)\n",
    "    \n",
    "    # Getting the difference of the orders and promotions between weeks t-1 and t-2...\n",
    "    shifting[f'orderSum_diff_{i}'] = shifting.groupby('itemID')[f'orderSum_{i}'].diff()\n",
    "    shifting[f'promotion_mean_diff_{i}'] = shifting.groupby('itemID')[f'promotion_mean_{i}'].diff()\n",
    "shifting.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum error\n",
    "The maximum error we could get in this dataset would be just guessing the mean of our sales from weeks 1 to 12, and that's what the cell below is computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing the mean of 'orderSum' for all items in target 90.29706562119341\n"
     ]
    }
   ],
   "source": [
    "worst_possible_prediction = shifting.loc[shifting.group_backwards < 13]['orderSum'].mean()\n",
    "prediction = np.full(shifting.loc[shifting.group_backwards == 13]['orderSum'].shape, worst_possible_prediction) # Array filled with the mean...\n",
    "target = shifting.loc[shifting.group_backwards == 13]['orderSum']\n",
    "print(\"Guessing the mean of 'orderSum' for all items in target\", mse(target, prediction) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "All my experiments will use weeks 13 to 3 as a train set, week 2 as our validation set and week 1 as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost requires that all columns should\n",
    "stringColumns = shifting.columns[3:]\n",
    "shifting[stringColumns] = shifting[stringColumns].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype conversion required by Catboost\n",
    "train = shifting.loc[shifting.group_backwards >= 3]\n",
    "val = shifting.loc[shifting.group_backwards == 2]\n",
    "test = shifting.loc[shifting.group_backwards == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I recommend to the other members of the team keeping the\n",
    "# datatypes of our datasets as Pandas DataFrames instead of Numpy,\n",
    "# since It will easier to use Boosting Analysis frameworks\n",
    "y_train = train['orderSum']\n",
    "y_val = val['orderSum']\n",
    "X_train = train.drop(columns=[\"orderSum\"])\n",
    "X_val = val.drop(columns=[\"orderSum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Pool\n",
    "train_pool = Pool(X_train, \n",
    "                  y_train, \n",
    "                  cat_features=[8,9,10])\n",
    "\n",
    "val_pool = Pool(X_val, \n",
    "                  y_val, \n",
    "                  cat_features=[8,9,10])\n",
    "# test_pool = Pool(test_data.astype(str), \n",
    "#                  cat_features=[8,9,10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 100.9859886\ttest: 105.3582307\tbest: 105.3582307 (0)\ttotal: 193ms\tremaining: 3m 12s\n",
      "1:\tlearn: 99.1420325\ttest: 102.6691635\tbest: 102.6691635 (1)\ttotal: 264ms\tremaining: 2m 11s\n",
      "2:\tlearn: 97.5669743\ttest: 99.9306748\tbest: 99.9306748 (2)\ttotal: 326ms\tremaining: 1m 48s\n",
      "3:\tlearn: 96.3818764\ttest: 98.6021707\tbest: 98.6021707 (3)\ttotal: 393ms\tremaining: 1m 37s\n",
      "4:\tlearn: 95.3057960\ttest: 97.1708837\tbest: 97.1708837 (4)\ttotal: 507ms\tremaining: 1m 40s\n",
      "5:\tlearn: 94.1458137\ttest: 95.6902170\tbest: 95.6902170 (5)\ttotal: 578ms\tremaining: 1m 35s\n",
      "6:\tlearn: 93.4752886\ttest: 94.9083519\tbest: 94.9083519 (6)\ttotal: 634ms\tremaining: 1m 30s\n",
      "7:\tlearn: 92.9225411\ttest: 94.2611453\tbest: 94.2611453 (7)\ttotal: 696ms\tremaining: 1m 26s\n",
      "8:\tlearn: 92.2531031\ttest: 93.4342248\tbest: 93.4342248 (8)\ttotal: 752ms\tremaining: 1m 22s\n",
      "9:\tlearn: 91.8245055\ttest: 93.0000419\tbest: 93.0000419 (9)\ttotal: 791ms\tremaining: 1m 18s\n",
      "10:\tlearn: 91.3963707\ttest: 92.6919112\tbest: 92.6919112 (10)\ttotal: 853ms\tremaining: 1m 16s\n",
      "11:\tlearn: 91.1096353\ttest: 92.4771670\tbest: 92.4771670 (11)\ttotal: 895ms\tremaining: 1m 13s\n",
      "12:\tlearn: 90.6041107\ttest: 91.6919674\tbest: 91.6919674 (12)\ttotal: 949ms\tremaining: 1m 12s\n",
      "13:\tlearn: 90.2365872\ttest: 91.3851761\tbest: 91.3851761 (13)\ttotal: 1.03s\tremaining: 1m 12s\n",
      "14:\tlearn: 89.8754573\ttest: 90.9518566\tbest: 90.9518566 (14)\ttotal: 1.09s\tremaining: 1m 11s\n",
      "15:\tlearn: 89.6104716\ttest: 90.8294246\tbest: 90.8294246 (15)\ttotal: 1.16s\tremaining: 1m 11s\n",
      "16:\tlearn: 89.1566091\ttest: 90.3418562\tbest: 90.3418562 (16)\ttotal: 1.21s\tremaining: 1m 10s\n",
      "17:\tlearn: 88.9932368\ttest: 90.1793909\tbest: 90.1793909 (17)\ttotal: 1.28s\tremaining: 1m 9s\n",
      "18:\tlearn: 88.7403603\ttest: 89.9992006\tbest: 89.9992006 (18)\ttotal: 1.34s\tremaining: 1m 9s\n",
      "19:\tlearn: 88.5977518\ttest: 89.8864913\tbest: 89.8864913 (19)\ttotal: 1.4s\tremaining: 1m 8s\n",
      "20:\tlearn: 88.2401225\ttest: 89.4479811\tbest: 89.4479811 (20)\ttotal: 1.46s\tremaining: 1m 8s\n",
      "21:\tlearn: 88.0813721\ttest: 89.4636703\tbest: 89.4479811 (20)\ttotal: 1.52s\tremaining: 1m 7s\n",
      "22:\tlearn: 87.9128547\ttest: 89.3686219\tbest: 89.3686219 (22)\ttotal: 1.58s\tremaining: 1m 7s\n",
      "23:\tlearn: 87.7979736\ttest: 89.3600012\tbest: 89.3600012 (23)\ttotal: 1.63s\tremaining: 1m 6s\n",
      "24:\tlearn: 87.6895175\ttest: 89.3613951\tbest: 89.3600012 (23)\ttotal: 1.68s\tremaining: 1m 5s\n",
      "25:\tlearn: 87.2182175\ttest: 89.3650446\tbest: 89.3600012 (23)\ttotal: 1.75s\tremaining: 1m 5s\n",
      "26:\tlearn: 87.1265030\ttest: 89.3990096\tbest: 89.3600012 (23)\ttotal: 1.8s\tremaining: 1m 4s\n",
      "27:\tlearn: 86.9979096\ttest: 89.3979954\tbest: 89.3600012 (23)\ttotal: 1.86s\tremaining: 1m 4s\n",
      "28:\tlearn: 86.7072623\ttest: 89.1535648\tbest: 89.1535648 (28)\ttotal: 1.92s\tremaining: 1m 4s\n",
      "29:\tlearn: 86.5847880\ttest: 89.1346613\tbest: 89.1346613 (29)\ttotal: 1.98s\tremaining: 1m 4s\n",
      "30:\tlearn: 86.5413201\ttest: 89.1092871\tbest: 89.1092871 (30)\ttotal: 2.03s\tremaining: 1m 3s\n",
      "31:\tlearn: 86.2886387\ttest: 88.9327334\tbest: 88.9327334 (31)\ttotal: 2.09s\tremaining: 1m 3s\n",
      "32:\tlearn: 86.2073241\ttest: 88.7308364\tbest: 88.7308364 (32)\ttotal: 2.15s\tremaining: 1m 3s\n",
      "33:\tlearn: 85.9704227\ttest: 88.7859271\tbest: 88.7308364 (32)\ttotal: 2.22s\tremaining: 1m 3s\n",
      "34:\tlearn: 85.7744250\ttest: 88.5777165\tbest: 88.5777165 (34)\ttotal: 2.27s\tremaining: 1m 2s\n",
      "35:\tlearn: 85.7129752\ttest: 88.5237681\tbest: 88.5237681 (35)\ttotal: 2.34s\tremaining: 1m 2s\n",
      "36:\tlearn: 85.4699997\ttest: 88.2296883\tbest: 88.2296883 (36)\ttotal: 2.4s\tremaining: 1m 2s\n",
      "37:\tlearn: 85.4371489\ttest: 88.2146857\tbest: 88.2146857 (37)\ttotal: 2.45s\tremaining: 1m 1s\n",
      "38:\tlearn: 85.2387681\ttest: 88.0996665\tbest: 88.0996665 (38)\ttotal: 2.5s\tremaining: 1m 1s\n",
      "39:\tlearn: 85.1433916\ttest: 88.1149036\tbest: 88.0996665 (38)\ttotal: 2.56s\tremaining: 1m 1s\n",
      "40:\tlearn: 85.0232494\ttest: 87.8549699\tbest: 87.8549699 (40)\ttotal: 2.61s\tremaining: 1m 1s\n",
      "41:\tlearn: 84.9533149\ttest: 87.8560775\tbest: 87.8549699 (40)\ttotal: 2.67s\tremaining: 1m\n",
      "42:\tlearn: 84.8600352\ttest: 87.8628812\tbest: 87.8549699 (40)\ttotal: 2.73s\tremaining: 1m\n",
      "43:\tlearn: 84.7332413\ttest: 87.7254435\tbest: 87.7254435 (43)\ttotal: 2.79s\tremaining: 1m\n",
      "44:\tlearn: 84.7032345\ttest: 87.7303870\tbest: 87.7254435 (43)\ttotal: 2.84s\tremaining: 1m\n",
      "45:\tlearn: 84.5942930\ttest: 87.6169658\tbest: 87.6169658 (45)\ttotal: 2.92s\tremaining: 1m\n",
      "46:\tlearn: 84.5097634\ttest: 87.5424879\tbest: 87.5424879 (46)\ttotal: 2.98s\tremaining: 1m\n",
      "47:\tlearn: 84.4462445\ttest: 87.4704513\tbest: 87.4704513 (47)\ttotal: 3.04s\tremaining: 1m\n",
      "48:\tlearn: 84.4145315\ttest: 87.4436759\tbest: 87.4436759 (48)\ttotal: 3.09s\tremaining: 1m\n",
      "49:\tlearn: 84.2872172\ttest: 87.2683205\tbest: 87.2683205 (49)\ttotal: 3.15s\tremaining: 59.8s\n",
      "50:\tlearn: 83.8184176\ttest: 87.3569482\tbest: 87.2683205 (49)\ttotal: 3.2s\tremaining: 59.6s\n",
      "51:\tlearn: 83.7033484\ttest: 87.2812330\tbest: 87.2683205 (49)\ttotal: 3.25s\tremaining: 59.2s\n",
      "52:\tlearn: 83.6153250\ttest: 87.2892935\tbest: 87.2683205 (49)\ttotal: 3.29s\tremaining: 58.9s\n",
      "53:\tlearn: 83.5746594\ttest: 87.2799421\tbest: 87.2683205 (49)\ttotal: 3.33s\tremaining: 58.4s\n",
      "54:\tlearn: 83.5005936\ttest: 87.2685650\tbest: 87.2683205 (49)\ttotal: 3.39s\tremaining: 58.3s\n",
      "Stopped by overfitting detector  (5 iterations wait)\n",
      "\n",
      "bestTest = 87.2683205\n",
      "bestIteration = 49\n",
      "\n",
      "Shrink model to first 50 iterations.\n"
     ]
    }
   ],
   "source": [
    "# specify the training parameters \n",
    "model = CatBoostRegressor(depth=6, \n",
    "                          learning_rate=0.1, \n",
    "                          loss_function='RMSE',\n",
    "                          early_stopping_rounds=5)\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,\n",
    "    logging_level='Verbose',  # you can uncomment this for text output\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting at test time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['orderSum']\n",
    "X_test = test.drop(columns=[\"orderSum\"])\n",
    "final_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1810,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions[final_predictions < 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating our Kaggle CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.Series(0, index=np.arange(1, len(items)+1))\n",
    "final[items.itemID] = final_predictions.astype(int)\n",
    "\n",
    "final.to_csv(\"cat_kaggle_df.csv\", header=[\"demandPrediction\"],\n",
    "            index_label=\"itemID\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving our model in disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%d-%m-%Y-%Hh%Mm%Ss\")\n",
    "modelName = 'cat-' + now\n",
    "bst.save_model(modelName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
