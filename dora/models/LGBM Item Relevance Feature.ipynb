{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM - Item Relevance Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0v.zip\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "\n",
    "NUMBER_OF_LAGS = 2\n",
    "\n",
    "sys.path.append(\"../../main/datasets/\")\n",
    "!ls  ../../main/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_score(prediction, target, simulatedPrice):\n",
    "    prediction = prediction.astype(int)\n",
    "\n",
    "    return np.sum((prediction - np.maximum(prediction - target, 0) * 1.6)  * simulatedPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval(prediction, dtrain):\n",
    "    \n",
    "    prediction = prediction.astype(int)\n",
    "    target = dtrain.get_label()\n",
    "\n",
    "    simulatedPrice = dtrain.get_weight()\n",
    "    \n",
    "    return 'feval', np.sum((prediction - np.maximum(prediction - target, 0) * 1.6)  * simulatedPrice), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(predt, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    sp = dtrain.get_weight()\n",
    "    return -2 * (predt - np.maximum(predt - y, 0) * 1.6) * (1 - (predt > y) * 1.6) * sp\n",
    "\n",
    "def hessian(predt, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    sp = dtrain.get_weight() \n",
    "    return -2 * ((1 - (predt > y) * 1.6) ** 2) * sp\n",
    "\n",
    "def objective(predt, dtrain):\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our dataset\n",
    "This notebook makes this step cleaner than the previous versions. So It'll be tidier and shorter than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checks... (10463, 3) (10463, 8) (2181955, 5)\n"
     ]
    }
   ],
   "source": [
    "infos, items, orders = read_data(\"../../main/datasets/\")\n",
    "print(\"Sanity checks...\", infos.shape, items.shape, orders.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing our time signatures\n",
    "process_time(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_builder(orders, items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage_accum_cat_3 feature...\n",
    "df = cumulative_sale_by_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding our weeks as a series of sines and cosines...\n",
    "# This function will consider our period as a semester in a year,\n",
    "# so we can try other types of time encoding later!\n",
    "df = time_encoder(df, 'group_backwards', 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifting = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates rolling-window features based on 'orderSum' in our dataset!\n",
    "item_group = shifting.groupby([\"itemID\", \"group_backwards\"]).agg({'orderSum':'sum'})\n",
    "\n",
    "# We'll .shift(-1) because it sorts our \"group_backwards\", \n",
    "# so doing .shift(1) would cause a HUGE dataleak.\n",
    "aux_shifting = item_group.groupby('itemID')[['orderSum']].shift(-1)\n",
    "\n",
    "aux_shifting.sort_values(['itemID', 'group_backwards'], ascending=[True, False], inplace=True)\n",
    "\n",
    "for i in range(3):\n",
    "    rolled_window = aux_shifting.groupby(['itemID'], as_index=False)[['orderSum']].rolling(2 ** i).mean()\n",
    "    rolled_window.rename(columns={'orderSum':f\"orderSum_mean_rolled_{i}\"}, inplace=True)\n",
    "    shifting = pd.merge(shifting, rolled_window, left_on=['itemID', 'group_backwards'], right_on=['itemID', 'group_backwards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell adds to our dataset two features: \"itemRelevance\" and \"soldUntilThisWeek\"\n",
    "# 'soldUntilThisWeek' = weeksThatAGivenItemSold / numberOfPastWeeks\n",
    "# 'itemRelevance' = 'soldUntilThisWeek' * 'recomendedRetailPrice'\n",
    "df_copy = df.copy()\n",
    "\n",
    "df_copy['soldUntilThisWeek'] = 0\n",
    "df_copy.loc[df.orderSum > 0, 'soldUntilThisWeek'] = 1\n",
    "\n",
    "group_item_sold = df_copy.groupby(['group_backwards', 'itemID']).agg({'soldUntilThisWeek':'sum'})\n",
    "\n",
    "# Taking infos from the past to the present date with shifting...\n",
    "# We'll .shift(-1) because it sorts our \"group_backwards\", \n",
    "# so doing .shift(1) would cause a HUGE dataleak.\n",
    "test = group_item_sold.groupby('itemID')[['soldUntilThisWeek']].shift(-1)\n",
    "test.sort_values(['itemID', 'group_backwards'], ascending=[True, False], inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "aux = pd.DataFrame()\n",
    "for i in range(13, 0, -1):\n",
    "    accum = test.loc[test.group_backwards >= i].groupby(['itemID'], as_index=False).agg({'soldUntilThisWeek':'sum'})\n",
    "    accum['group_backwards'] = i\n",
    "    accum['soldUntilThisWeek'] = accum['soldUntilThisWeek'] / (13 - i)\n",
    "    aux = pd.concat([aux, accum])\n",
    "    \n",
    "aux.reset_index(0, drop=True, inplace=True)\n",
    "\n",
    "new_feature = pd.merge(shifting, aux, left_on=['group_backwards', 'itemID'], right_on=['group_backwards', 'itemID'])\n",
    "new_feature['itemRelevance'] = new_feature['recommendedRetailPrice'] * new_feature['soldUntilThisWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifting = new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell lags and diffs our feature 'orderSum'\n",
    "for i in range(1, NUMBER_OF_LAGS + 1):\n",
    "    # Carrying the data of weeks t-1 and t-2\n",
    "    shifting[f'orderSum_{i}'] = shifting.groupby('itemID')['orderSum'].shift(i)\n",
    "    shifting[f'soldUntilThisWeek_{-i}'] = shifting.groupby('itemID')['soldUntilThisWeek'].shift(i)\n",
    "    shifting[f'itemRelevance_{-i}'] = shifting.groupby('itemID')['itemRelevance'].shift(i)\n",
    "    \n",
    "    # Getting the difference between weeks t-1 and t-2...\n",
    "    shifting[f'orderSum_diff_{i}'] = shifting.groupby('itemID')[f'orderSum_{i}'].diff()\n",
    "    shifting[f'soldUntilThisWeek_diff_{-i}'] = shifting.groupby('itemID')[f'soldUntilThisWeek_{-i}'].shift(i)\n",
    "    shifting[f'itemRelevance_{-i}'] = shifting.groupby('itemID')[f'itemRelevance_{-i}'].shift(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_backwards</th>\n",
       "      <th>itemID</th>\n",
       "      <th>orderSum</th>\n",
       "      <th>brand</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>customerRating</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>recommendedRetailPrice</th>\n",
       "      <th>...</th>\n",
       "      <th>orderSum_1</th>\n",
       "      <th>soldUntilThisWeek_-1</th>\n",
       "      <th>itemRelevance_-1</th>\n",
       "      <th>orderSum_diff_1</th>\n",
       "      <th>soldUntilThisWeek_diff_-1</th>\n",
       "      <th>orderSum_2</th>\n",
       "      <th>soldUntilThisWeek_-2</th>\n",
       "      <th>itemRelevance_-2</th>\n",
       "      <th>orderSum_diff_2</th>\n",
       "      <th>soldUntilThisWeek_diff_-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16.92</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15.89</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40.17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.04</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136014</th>\n",
       "      <td>1</td>\n",
       "      <td>10459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180</td>\n",
       "      <td>253</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>56.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>5.657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136015</th>\n",
       "      <td>1</td>\n",
       "      <td>10460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>163.81</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>16.381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20.47625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136016</th>\n",
       "      <td>1</td>\n",
       "      <td>10461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>128.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136017</th>\n",
       "      <td>1</td>\n",
       "      <td>10462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180</td>\n",
       "      <td>253</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>166.97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>16.697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136018</th>\n",
       "      <td>1</td>\n",
       "      <td>10463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>154.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136019 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        group_backwards  itemID  orderSum  brand  manufacturer  \\\n",
       "0                    13       1       0.0      0             1   \n",
       "1                    13       2       0.0      0             2   \n",
       "2                    13       3       1.0      0             3   \n",
       "3                    13       4       0.0      0             2   \n",
       "4                    13       5       2.0      0             2   \n",
       "...                 ...     ...       ...    ...           ...   \n",
       "136014                1   10459       0.0    180           253   \n",
       "136015                1   10460       0.0      0           253   \n",
       "136016                1   10461       0.0      0           253   \n",
       "136017                1   10462       0.0    180           253   \n",
       "136018                1   10463       0.0      0           253   \n",
       "\n",
       "        customerRating  category1  category2  category3  \\\n",
       "0                 4.38          1          1          1   \n",
       "1                 3.00          1          2          1   \n",
       "2                 5.00          1          3          1   \n",
       "3                 4.44          1          2          1   \n",
       "4                 2.33          1          1          1   \n",
       "...                ...        ...        ...        ...   \n",
       "136014            0.00          8         44          8   \n",
       "136015            0.00          8         44          8   \n",
       "136016            0.00          8         44          8   \n",
       "136017            0.00          8         44          8   \n",
       "136018            0.00          8         44          8   \n",
       "\n",
       "        recommendedRetailPrice  ...  orderSum_1  soldUntilThisWeek_-1  \\\n",
       "0                         8.84  ...         NaN                   NaN   \n",
       "1                        16.92  ...         NaN                   NaN   \n",
       "2                        15.89  ...         NaN                   NaN   \n",
       "3                        40.17  ...         NaN                   NaN   \n",
       "4                        17.04  ...         NaN                   NaN   \n",
       "...                        ...  ...         ...                   ...   \n",
       "136014                   56.57  ...         0.0              0.090909   \n",
       "136015                  163.81  ...         0.0              0.090909   \n",
       "136016                  128.01  ...         0.0              0.000000   \n",
       "136017                  166.97  ...         0.0              0.090909   \n",
       "136018                  154.82  ...         0.0              0.090909   \n",
       "\n",
       "        itemRelevance_-1  orderSum_diff_1  soldUntilThisWeek_diff_-1  \\\n",
       "0                    NaN              NaN                        NaN   \n",
       "1                    NaN              NaN                        NaN   \n",
       "2                    NaN              NaN                        NaN   \n",
       "3                    NaN              NaN                        NaN   \n",
       "4                    NaN              NaN                        NaN   \n",
       "...                  ...              ...                        ...   \n",
       "136014             5.657              0.0                        0.1   \n",
       "136015            16.381              0.0                        0.1   \n",
       "136016             0.000              0.0                        0.0   \n",
       "136017            16.697              0.0                        0.1   \n",
       "136018             0.000             -1.0                        0.0   \n",
       "\n",
       "        orderSum_2  soldUntilThisWeek_-2  itemRelevance_-2  orderSum_diff_2  \\\n",
       "0              NaN                   NaN               NaN              NaN   \n",
       "1              NaN                   NaN               NaN              NaN   \n",
       "2              NaN                   NaN               NaN              NaN   \n",
       "3              NaN                   NaN               NaN              NaN   \n",
       "4              NaN                   NaN               NaN              NaN   \n",
       "...            ...                   ...               ...              ...   \n",
       "136014         0.0                   0.1           0.00000             -1.0   \n",
       "136015         0.0                   0.1          20.47625              0.0   \n",
       "136016         0.0                   0.0           0.00000              0.0   \n",
       "136017         0.0                   0.1           0.00000              0.0   \n",
       "136018         1.0                   0.0           0.00000              1.0   \n",
       "\n",
       "        soldUntilThisWeek_diff_-2  \n",
       "0                             NaN  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3                             NaN  \n",
       "4                             NaN  \n",
       "...                           ...  \n",
       "136014                      0.000  \n",
       "136015                      0.125  \n",
       "136016                      0.000  \n",
       "136017                      0.000  \n",
       "136018                      0.000  \n",
       "\n",
       "[136019 rows x 28 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = pd.DataFrame()\n",
    "for i in range(13, 0, -1):\n",
    "    accum = shifting.loc[df.group_backwards >= i].groupby(['itemID']).agg(orderSum_median=('orderSum','median'))\n",
    "    accum['group_backwards'] = i\n",
    "    aux = pd.concat([aux, accum])\n",
    "aux.reset_index(inplace=True)\n",
    "aux['orderSum_median'] = aux.groupby('itemID')['orderSum_median'].shift(1)\n",
    "shifting = pd.merge(shifting, aux, left_on=['itemID', 'group_backwards'], right_on=['itemID', 'group_backwards'])\n",
    "shifting['median_gap'] = shifting['orderSum_1'] - shifting['orderSum_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM Says on docs that it automatically handles zero values as NaN,\n",
    "# so we'll keep this standard...\n",
    "shifting.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum error\n",
    "The maximum error we could get in this dataset would be just guessing the mean of our sales from weeks 1 to 12, and that's what the cell below is computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing the mean of 'orderSum' for all items in target 90.29706562119341\n"
     ]
    }
   ],
   "source": [
    "worst_possible_prediction = shifting.loc[shifting.group_backwards < 13]['orderSum'].mean()\n",
    "prediction = np.full(shifting.loc[shifting.group_backwards == 13]['orderSum'].shape, worst_possible_prediction) # Array filled with the mean...\n",
    "target = shifting.loc[shifting.group_backwards == 13]['orderSum']\n",
    "print(\"Guessing the mean of 'orderSum' for all items in target\", mse(target, prediction) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting (Train until week 3 / Val. week 2/ Test week 1)\n",
    "All my experiments will use weeks 13 to 3 as a train set, week 2 as our validation set and week 1 as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shifting.loc[shifting.group_backwards >= 3]\n",
    "val = shifting.loc[shifting.group_backwards == 2]\n",
    "test = shifting.loc[shifting.group_backwards == 1]\n",
    "\n",
    "weights = infos.set_index('itemID')['simulationPrice'].to_dict()\n",
    "\n",
    "w_train = train['itemID'].map(weights)\n",
    "w_val = val['itemID'].map(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I recommend to the other members of the team keeping the\n",
    "# datatypes of our datasets as Pandas DataFrames instead of Numpy,\n",
    "# since It will easier to use Boosting Analysis frameworks\n",
    "y_train = train['orderSum']\n",
    "y_val = val['orderSum']\n",
    "X_train = train.drop(columns=[\"orderSum\"])\n",
    "X_val = val.drop(columns=[\"orderSum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n",
      "[5]\ttraining's rmse: 40.3085\ttraining's feval: 602433\tvalid_1's rmse: 43.6388\tvalid_1's feval: 74006.1\n",
      "[10]\ttraining's rmse: 40.2735\ttraining's feval: 771823\tvalid_1's rmse: 43.5891\tvalid_1's feval: 97000.8\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's rmse: 40.2786\ttraining's feval: 720139\tvalid_1's rmse: 43.5979\tvalid_1's feval: 98986.6\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "#           \"objective\" : \"poisson\",\n",
    "          \"objective\" : \"l1\",\n",
    "          \"metric\" :\"rmse\",\n",
    "          \"learning_rate\" : 0.5,\n",
    "          'verbosity': 1,\n",
    "          'max_depth': 76,\n",
    "          'num_leaves': 30,\n",
    "          \"min_data_in_leaf\":1000,\n",
    "         }\n",
    "\n",
    "\n",
    "lgbtrain = lgb.Dataset(X_train, label = y_train, weight=w_train, categorical_feature=[6,7,8])\n",
    "lgbvalid = lgb.Dataset(X_val, label = y_val, weight=w_val, categorical_feature=[6,7,8])\n",
    "\n",
    "num_round = 1000\n",
    "model = lgb.train(params,\n",
    "                  lgbtrain,\n",
    "                  num_round,\n",
    "                  valid_sets = [lgbtrain, lgbvalid], \n",
    "                  verbose_eval=5,\n",
    "                  early_stopping_rounds=5,\n",
    "#                   fobj=objective,\n",
    "                  feval=feval,\n",
    "#                   callbacks=[lgb.reset_parameter(learning_rate=lambda current_round: alpha * np.e**(k * current_round))]\n",
    "                  \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category2', 'group_backwards', 'itemID', 'manufacturer',\n",
       "       'orderSum_mean_rolled_2', 'orderSum_mean_rolled_1',\n",
       "       'group_backwards_sin', 'percentage_accum_cat_3', 'soldUntilThisWeek',\n",
       "       'brand', 'itemRelevance', 'customerRating', 'orderSum_diff_1',\n",
       "       'orderSum_median', 'orderSum_diff_2', 'orderSum_mean_rolled_0',\n",
       "       'soldUntilThisWeek_-1', 'itemRelevance_-1', 'soldUntilThisWeek_diff_-2',\n",
       "       'orderSum_2', 'soldUntilThisWeek_diff_-1', 'category1', 'median_gap',\n",
       "       'category3', 'itemRelevance_-2', 'soldUntilThisWeek_-2',\n",
       "       'recommendedRetailPrice', 'orderSum_1', 'group_backwards_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[list(reversed(model.feature_importance().argsort()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting (Train until week 2 and test with week 1)\n",
    "All my experiments will use weeks 13 to 2 as a train set and week 1 as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shifting.loc[shifting.group_backwards >= 2]\n",
    "val = shifting.loc[shifting.group_backwards == 1]\n",
    "test = shifting.loc[shifting.group_backwards == 1]\n",
    "\n",
    "weights = infos.set_index('itemID')['simulationPrice'].to_dict()\n",
    "\n",
    "w_train = train['itemID'].map(weights)\n",
    "w_val = val['itemID'].map(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I recommend to the other members of the team keeping the\n",
    "# datatypes of our datasets as Pandas DataFrames instead of Numpy,\n",
    "# since It will easier to use Boosting Analysis frameworks\n",
    "y_train = train['orderSum']\n",
    "y_val = val['orderSum']\n",
    "X_train = train.drop(columns=[\"orderSum\"])\n",
    "X_val = val.drop(columns=[\"orderSum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n",
      "[5]\ttraining's rmse: 40.3084\ttraining's feval: 567297\tvalid_1's rmse: 43.6407\tvalid_1's feval: 72622.2\n",
      "[10]\ttraining's rmse: 40.2874\ttraining's feval: 649069\tvalid_1's rmse: 43.6131\tvalid_1's feval: 83109.1\n",
      "[15]\ttraining's rmse: 40.2864\ttraining's feval: 653121\tvalid_1's rmse: 43.6138\tvalid_1's feval: 82690\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's rmse: 40.2874\ttraining's feval: 649069\tvalid_1's rmse: 43.6131\tvalid_1's feval: 83109.1\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "#           \"objective\" : \"poisson\",\n",
    "          \"objective\" : \"l1\",\n",
    "          \"metric\" :\"rmse\",\n",
    "          \"learning_rate\" : 0.5,\n",
    "          'verbosity': 1,\n",
    "          'max_depth': 5,\n",
    "          'num_leaves': 19,\n",
    "          \"min_data_in_leaf\":3000,\n",
    "         }\n",
    "\n",
    "lgbtrain = lgb.Dataset(X_train, label = y_train, weight=w_train)\n",
    "lgbvalid = lgb.Dataset(X_val, label = y_val, weight=w_val)\n",
    "\n",
    "num_round = 1000\n",
    "model = lgb.train(params,\n",
    "                  lgbtrain,\n",
    "                  num_round,\n",
    "                  valid_sets = [lgbtrain, lgbvalid], \n",
    "                  verbose_eval=5,\n",
    "                  early_stopping_rounds=5,\n",
    "#                   fobj=objective,\n",
    "                  feval=feval,\n",
    "                  \n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting at test time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['orderSum']\n",
    "X_test = test.drop(columns=[\"orderSum\"])\n",
    "final_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.  , 0.75, ..., 1.  , 0.  , 1.  ])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions[final_predictions < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98986.63799999999"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_score(final_predictions, y_test.values, infos['simulationPrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating our Kaggle CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.Series(0, index=np.arange(1, len(items)+1))\n",
    "final[items.itemID] = final_predictions.astype(int)\n",
    "\n",
    "final.to_csv(\"lgbm_kaggle_df.csv\", header=[\"demandPrediction\"],\n",
    "            index_label=\"itemID\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving our model in disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%d-%m-%Y-%Hh%Mm%Ss\")\n",
    "modelName = 'lgbm-' + now\n",
    "xgb.save_model(modelName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitb3ebfd1fa0594a1c9d5c617333c2c1a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
